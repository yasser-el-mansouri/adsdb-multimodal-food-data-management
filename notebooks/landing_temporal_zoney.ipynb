{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c522da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yasse\\Documents\\UNI\\Master\\DS\\ADSDB\\project\\adsdb-multimodal-food-data-management\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import  load_dataset\n",
    "from huggingface_hub import HfApi, hf_hub_url\n",
    "import s3fs\n",
    "from os.path import basename\n",
    "import requests\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5782ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO re do code well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f673ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN=os.getenv(\"HF_TOKEN\")\n",
    "HF_ORGA=os.getenv(\"HF_ORGA\")\n",
    "HF_DATASET=os.getenv(\"HF_DATASET\")\n",
    "HF_REV=os.getenv(\"HF_REV\")\n",
    "MINIO_PASSWORD=os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_USER=os.getenv(\"MINIO_USER\")\n",
    "MINIO_ENDPOINT=os.getenv(\"MINIO_ENDPOINT\")\n",
    "MINIO_BUCKET=os.getenv(\"MINIO_LANDING_BUCKET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0481fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "info = api.whoami(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a60e21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(\n",
    "    key=MINIO_USER,\n",
    "    secret=MINIO_PASSWORD,\n",
    "    client_kwargs={\"endpoint_url\": MINIO_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e32e1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = api.list_datasets(author=HF_ORGA, token=HF_TOKEN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf4321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Contenido de det_ingrs.json (ADSDB-DYS/adsdb-multimodal-food-data-management):\n"
     ]
    }
   ],
   "source": [
    "SKIP = {\".gitattributes\", \".gitignore\", \".gitkeep\"}\n",
    "\n",
    "for ds in datasets:\n",
    "    ds_id=ds.id\n",
    "    files=api.list_repo_files(repo_id=ds_id, repo_type=\"dataset\", revision=HF_REV)\n",
    "    for path in files:\n",
    "        fname = basename(path)\n",
    "        if fname.startswith(\".\") or fname.endswith(\".tar\") or fname in SKIP:\n",
    "            continue\n",
    "        url=hf_hub_url(repo_id=ds_id, filename=path, repo_type=\"dataset\", revision=HF_REV)\n",
    "        r=requests.get(url, stream=True, headers={\"authorization\":f\"Bearer {HF_TOKEN}\"})\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        if fname.endswith(\".json\"):\n",
    "            content = r.content.decode(\"utf-8\")\n",
    "            try:\n",
    "                data = json.loads(content)\n",
    "                print(f\"\\nüìÑ Contenido de {fname} ({ds_id}): \")\n",
    "                print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"\\n‚ö†Ô∏è No se pudo parsear {fname} como JSON:\")\n",
    "                print(content[:500])  # mostrar los primeros caracteres\n",
    "        \n",
    "        with fs.open(f\"{MINIO_BUCKET}/temporal_landing/{path.replace('/', '__')}\",\"wb\") as f: # TODO many files in folders can result in the same, see and ask the best way to do it\n",
    "            for chunk in r.iter_content(1024*1024):\n",
    "                if chunk: f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb22fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ijson\n",
      "  Downloading ijson-3.4.0-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\yasse\\documents\\uni\\master\\ds\\adsdb\\project\\adsdb-multimodal-food-data-management\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\yasse\\documents\\uni\\master\\ds\\adsdb\\project\\adsdb-multimodal-food-data-management\\.venv\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yasse\\documents\\uni\\master\\ds\\adsdb\\project\\adsdb-multimodal-food-data-management\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yasse\\documents\\uni\\master\\ds\\adsdb\\project\\adsdb-multimodal-food-data-management\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yasse\\documents\\uni\\master\\ds\\adsdb\\project\\adsdb-multimodal-food-data-management\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yasse\\documents\\uni\\master\\ds\\adsdb\\project\\adsdb-multimodal-food-data-management\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Downloading ijson-3.4.0-cp312-cp312-win_amd64.whl (54 kB)\n",
      "Installing collected packages: ijson\n",
      "Successfully installed ijson-3.4.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install ijson requests pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO see the other files to enrich data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63deaa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Dataset: ADSDB-DYS/adsdb-multimodal-food-data-management\n",
      "   ‚Ü≥ layer1 en MinIO: landing-zone/temporal_landing/recipe1M_layers__layer1.json\n",
      "   ‚Ü≥ layer2 en MinIO: landing-zone/temporal_landing/recipe1M_layers__layer2.json\n",
      "\n",
      "üîó Cruzando layer1 ‚Üî layer2 para ADSDB-DYS/adsdb-multimodal-food-data-management (tomando 50 recetas)‚Ä¶\n",
      "‚¨ÜÔ∏è  Subiendo im√°genes (solo recetas con im√°genes)‚Ä¶\n",
      "‚ö†Ô∏è Error subiendo imagen desde https://img-global.cpcdn.com/001_recipes/5806945844854784/0x0/photo.jpg -> landing-zone/temporal_landing/train_6bdca6e490.jpg: 400 Client Error: Bad Request for url: https://img-global.cpcdn.com/001_recipes/5806945844854784/0x0/photo.jpg\n",
      "‚ö†Ô∏è Error subiendo imagen desde https://img-global.cpcdn.com/001_recipes/5205549177110528/0x0/photo.jpg -> landing-zone/temporal_landing/train_f480145da5.jpg: 400 Client Error: Bad Request for url: https://img-global.cpcdn.com/001_recipes/5205549177110528/0x0/photo.jpg\n",
      "‚ö†Ô∏è Error subiendo imagen desde https://img-global.cpcdn.com/001_photo_reports/4904504583520256/0x0/photo.jpg -> landing-zone/temporal_landing/train_4636c7f576.jpg: 400 Client Error: Bad Request for url: https://img-global.cpcdn.com/001_photo_reports/4904504583520256/0x0/photo.jpg\n",
      "‚ö†Ô∏è Error subiendo imagen desde http://assets.kraftfoods.com/recipe_images/182098-182099.jpg -> landing-zone/temporal_landing/train_4247c11be3.jpg: HTTPConnectionPool(host='assets.kraftfoods.com', port=80): Max retries exceeded with url: /recipe_images/182098-182099.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x0000023BD5AABFE0>: Failed to resolve 'assets.kraftfoods.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "‚ö†Ô∏è Error subiendo imagen desde https://img-global.cpcdn.com/001_photo_reports/6008901292523520/0x0/photo.jpg -> landing-zone/temporal_landing/val_665bbeafc7.jpg: 400 Client Error: Bad Request for url: https://img-global.cpcdn.com/001_photo_reports/6008901292523520/0x0/photo.jpg\n",
      "‚ö†Ô∏è Error subiendo imagen desde https://img-global.cpcdn.com/001_photo_reports/4743408963289088/0x0/photo.jpg -> landing-zone/temporal_landing/val_87aea5035b.jpg: 400 Client Error: Bad Request for url: https://img-global.cpcdn.com/001_photo_reports/4743408963289088/0x0/photo.jpg\n",
      "‚ö†Ô∏è Error subiendo imagen desde http://assets.kraftfoods.com/recipe_images/Popcorn-Trail-Mix-61178.jpg -> landing-zone/temporal_landing/train_6f50459f10.jpg: HTTPConnectionPool(host='assets.kraftfoods.com', port=80): Max retries exceeded with url: /recipe_images/Popcorn-Trail-Mix-61178.jpg (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x0000023BD6860A10>: Failed to resolve 'assets.kraftfoods.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "‚ö†Ô∏è Error subiendo imagen desde https://img-global.cpcdn.com/001_recipes/4558674072698880/0x0/photo.jpg -> landing-zone/temporal_landing/train_ccfd7a7e7e.jpg: 400 Client Error: Bad Request for url: https://img-global.cpcdn.com/001_recipes/4558674072698880/0x0/photo.jpg\n",
      "‚úÖ ADSDB-DYS/adsdb-multimodal-food-data-management: Im√°genes subidas: 17\n",
      "üìÇ JSON usados desde MinIO:\n",
      "   - landing-zone/temporal_landing/recipe1M_layers__layer1.json\n",
      "   - landing-zone/temporal_landing/recipe1M_layers__layer2.json\n",
      "üìÇ Im√°genes guardadas en: landing-zone/temporal_landing/partition_a_b_c_d_<image_id>.jpg\n"
     ]
    }
   ],
   "source": [
    "# TODO temporal landing zone\n",
    "# --- Imports ---\n",
    "import os, io, json, requests, ijson\n",
    "from os.path import basename\n",
    "from huggingface_hub import hf_hub_url\n",
    "\n",
    "# --- Par√°metros ---\n",
    "SKIP = {\".gitattributes\", \".gitignore\", \".gitkeep\"}\n",
    "TIMEOUT = 60\n",
    "N_RECETAS = 50          # cu√°ntas recetas tomar de layer1 para cruzar\n",
    "IMGS_POR_RECETA = 5     # m√°x im√°genes por receta a subir\n",
    "BASE_DIR_MINIO = f\"{MINIO_BUCKET}/temporal_landing\"  # donde guardas todo\n",
    "\n",
    "headers = {\"authorization\": f\"Bearer {HF_TOKEN}\"} if HF_TOKEN else {}\n",
    "\n",
    "# --- Utils generales ---\n",
    "def _get(url, headers, timeout=TIMEOUT):\n",
    "    r = requests.get(url, stream=True, headers=headers, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def save_to_minio_stream(url, dst_path):\n",
    "    \"\"\"Descarga por streaming y guarda en MinIO en dst_path.\"\"\"\n",
    "    r = _get(url, headers)\n",
    "    with fs.open(dst_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    r.close()\n",
    "\n",
    "def underscored_img_path(partition, image_id):\n",
    "\n",
    "    base = image_id.split(\".\")[0]\n",
    "    return f\"{partition}_{image_id}\"\n",
    "\n",
    "def pick_n_from_layer1_fileobj(fileobj, n):\n",
    "    \"\"\"\n",
    "    Lee layer1.json desde un file-like (fs.open en MinIO) y devuelve:\n",
    "      recetas: [{id, title, ingredients, partition}]\n",
    "      id2partition: {id: partition}\n",
    "    \"\"\"\n",
    "    recetas, id2partition = [], {}\n",
    "    for item in ijson.items(fileobj, 'item'):\n",
    "        rid = item.get(\"id\")\n",
    "        if not rid:\n",
    "            continue\n",
    "        rec = {\n",
    "            \"id\": rid,\n",
    "            \"title\": item.get(\"title\"),\n",
    "            \"ingredients\": [x.get(\"text\") for x in item.get(\"ingredients\", []) if isinstance(x, dict) and \"text\" in x],\n",
    "            \"partition\": item.get(\"partition\"),\n",
    "        }\n",
    "        recetas.append(rec)\n",
    "        id2partition[rid] = rec[\"partition\"]\n",
    "        if len(recetas) >= n:\n",
    "            break\n",
    "    return recetas, id2partition\n",
    "\n",
    "def map_images_for_ids_fileobj(fileobj, ids, max_imgs_per_id=IMGS_POR_RECETA):\n",
    "    \"\"\"\n",
    "    Lee layer2.json desde un file-like y devuelve:\n",
    "      { id: [ {id: image_id, url: url}, ... ] }\n",
    "    Solo llena las que est√°n en 'ids'; se corta cuando encuentra todas.\n",
    "    \"\"\"\n",
    "    target = set(ids)\n",
    "    result = {rid: [] for rid in ids}\n",
    "    for item in ijson.items(fileobj, 'item'):\n",
    "        rid = item.get(\"id\")\n",
    "        if rid in target:\n",
    "            lst = []\n",
    "            for im in item.get(\"images\", []):\n",
    "                iid = im.get(\"id\")\n",
    "                u   = im.get(\"url\")\n",
    "                if iid and u:\n",
    "                    lst.append({\"id\": iid, \"url\": u})\n",
    "                if len(lst) >= max_imgs_per_id:\n",
    "                    break\n",
    "            result[rid] = lst\n",
    "            target.discard(rid)\n",
    "            if not target:\n",
    "                break\n",
    "    return result\n",
    "\n",
    "def upload_image_to_minio(url, dst_path):\n",
    "    \"\"\"Descarga y sube una imagen a MinIO en dst_path.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, stream=True, timeout=TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        with fs.open(dst_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        r.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error subiendo imagen desde {url} -> {dst_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Descarga TODO del/los datasets y detecta layer1/layer2 ---\n",
    "# Guardamos mapping por dataset: ds_id -> {'layer1': minio_path, 'layer2': minio_path}\n",
    "layers_paths = {}\n",
    "\n",
    "for ds in datasets:\n",
    "    ds_id = ds.id\n",
    "    print(f\"\\nüì¶ Dataset: {ds_id}\")\n",
    "    files = api.list_repo_files(repo_id=ds_id, repo_type=\"dataset\", revision=HF_REV)\n",
    "\n",
    "    # Para registrar si encontramos layer1/layer2 en este ds\n",
    "    l1_minio = None\n",
    "    l2_minio = None\n",
    "\n",
    "    for path in files:\n",
    "        fname = basename(path)\n",
    "        if fname.startswith(\".\") or fname.endswith(\".tar\") or fname in SKIP:\n",
    "            continue\n",
    "\n",
    "        # 1) Construye URL HF y descarga a MinIO (sustituyendo '/' por '__')\n",
    "        url = hf_hub_url(repo_id=ds_id, filename=path, repo_type=\"dataset\", revision=HF_REV)\n",
    "        try:\n",
    "            r = _get(url, headers)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è No se pudo abrir {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        minio_name = path.replace(\"/\", \"__\")\n",
    "        dst_path = f\"{BASE_DIR_MINIO}/{minio_name}\"\n",
    "        with fs.open(dst_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        r.close()\n",
    "\n",
    "        # 2) ¬øEs layer1/layer2? Guardamos la ruta en MinIO\n",
    "        # Priorizamos las que est√©n bajo 'recipe1M_layers/' si hay varias coincidencias\n",
    "        normalized = path.lower()\n",
    "        if normalized.endswith(\"/layer1.json\") or normalized == \"layer1.json\":\n",
    "            if (\"/recipe1m_layers/\" in normalized and (l1_minio is None or \"recipe1m_layers\" not in l1_minio.lower())) or l1_minio is None:\n",
    "                l1_minio = dst_path\n",
    "        if normalized.endswith(\"/layer2.json\") or normalized == \"layer2.json\":\n",
    "            if (\"/recipe1m_layers/\" in normalized and (l2_minio is None or \"recipe1m_layers\" not in l2_minio.lower())) or l2_minio is None:\n",
    "                l2_minio = dst_path\n",
    "\n",
    "    if l1_minio or l2_minio:\n",
    "        layers_paths[ds_id] = {\"layer1\": l1_minio, \"layer2\": l2_minio}\n",
    "        print(f\"   ‚Ü≥ layer1 en MinIO: {l1_minio}\")\n",
    "        print(f\"   ‚Ü≥ layer2 en MinIO: {l2_minio}\")\n",
    "\n",
    "# --- Para cada dataset con layer1 y layer2, cruzamos y subimos IM√ÅGENES ---\n",
    "for ds_id, paths in layers_paths.items():\n",
    "    layer1_minio_path = paths.get(\"layer1\")\n",
    "    layer2_minio_path = paths.get(\"layer2\")\n",
    "\n",
    "    if not layer1_minio_path or not layer2_minio_path:\n",
    "        print(f\"\\n‚è≠Ô∏è  Saltando {ds_id}: faltan layer1 o layer2 en MinIO.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüîó Cruzando layer1 ‚Üî layer2 para {ds_id} (tomando {N_RECETAS} recetas)‚Ä¶\")\n",
    "\n",
    "    # 1) Pick N recetas de layer1 (desde el archivo en MinIO, no desde HF)\n",
    "    with fs.open(layer1_minio_path, \"rb\") as f1:\n",
    "        recetas, id2partition = pick_n_from_layer1_fileobj(f1, N_RECETAS)\n",
    "\n",
    "    # 2) Mapear im√°genes para esas recetas desde layer2 (archivo en MinIO)\n",
    "    ids = [r[\"id\"] for r in recetas]\n",
    "    with fs.open(layer2_minio_path, \"rb\") as f2:\n",
    "        imgs_map = map_images_for_ids_fileobj(f2, ids, IMGS_POR_RECETA)\n",
    "\n",
    "    # 3) Subir SOLO las recetas que tienen al menos una imagen\n",
    "    print(\"‚¨ÜÔ∏è  Subiendo im√°genes (solo recetas con im√°genes)‚Ä¶\")\n",
    "    uploaded_count = 0\n",
    "    for rec in recetas:\n",
    "        rid = rec[\"id\"]\n",
    "        part = (rec.get(\"partition\") or id2partition.get(rid) or \"unknown\").lower()\n",
    "        imgs = imgs_map.get(rid, [])\n",
    "        if not imgs:\n",
    "            continue\n",
    "\n",
    "        for im in imgs:\n",
    "            iid = im[\"id\"]\n",
    "            url = im[\"url\"]\n",
    "            filename = underscored_img_path(partition=part, image_id=iid)\n",
    "            img_dst = f\"{BASE_DIR_MINIO}/{filename}\"\n",
    "            ok = upload_image_to_minio(url, img_dst)\n",
    "            if ok:\n",
    "                uploaded_count += 1\n",
    "\n",
    "    print(f\"‚úÖ {ds_id}: Im√°genes subidas: {uploaded_count}\")\n",
    "    print(f\"üìÇ JSON usados desde MinIO:\")\n",
    "    print(f\"   - {layer1_minio_path}\")\n",
    "    print(f\"   - {layer2_minio_path}\")\n",
    "    print(f\"üìÇ Im√°genes guardadas en: {BASE_DIR_MINIO}/partition_<image_id>.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91955414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO justify decisions like name convention, no history timestamp, folder structure, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
