# %% [markdown]
# # 1) Setup env, MinIO client, Chroma client

# %%
import os
from dotenv import load_dotenv, find_dotenv
import boto3
from botocore.config import Config
from typing import Dict, Any
from PIL import Image
import torch
import open_clip
from PIL import Image
from typing import Dict, Any
from PIL import Image
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction, SentenceTransformerEmbeddingFunction

dotenv_path = find_dotenv(filename='.env', usecwd=True)
if not dotenv_path:
    raise FileNotFoundError("Could not find .env. Set its path manually.")
print(f"Loading environment variables from {dotenv_path}")
load_dotenv(dotenv_path)

# --- ENV ---
TRUSTED_BUCKET       = os.environ.get("TRUSTED_BUCKET", "trusted-zone")

CHROMA_PERSIST_DIR   = os.environ.get("CHROMA_PERSIST_DIR", "exploitation_zone/chroma")

# --- MinIO S3 client ---
MINIO_USER     = os.environ.get("MINIO_USER")
MINIO_PASSWORD = os.environ.get("MINIO_PASSWORD")
MINIO_ENDPOINT = os.environ.get("MINIO_ENDPOINT")

session = boto3.session.Session(
    aws_access_key_id=MINIO_USER,
    aws_secret_access_key=MINIO_PASSWORD,
    region_name="us-east-1"
)
s3 = session.client(
    "s3",
    endpoint_url=MINIO_ENDPOINT,
    config=Config(signature_version="s3v4", s3={"addressing_style": "path"})
)

# --- Chroma client ---
CHROMA = PersistentClient(path="../" + CHROMA_PERSIST_DIR)

# %%
# Test MinIO connection and bucket
try:
    # List all buckets
    response = s3.list_buckets()
    print("Available buckets:", [b['Name'] for b in response['Buckets']])
    
    # Check if trusted-zone bucket exists
    if TRUSTED_BUCKET in [b['Name'] for b in response['Buckets']]:
        print(f"✅ Bucket '{TRUSTED_BUCKET}' exists")
    else:
        print(f"❌ Bucket '{TRUSTED_BUCKET}' does NOT exist")
        
    # Try to list objects in the bucket
    try:
        objects = s3.list_objects_v2(Bucket=TRUSTED_BUCKET)
        print(f"Objects in {TRUSTED_BUCKET}: {objects.get('KeyCount', 0)}")
    except Exception as e:
        print(f"Cannot access bucket {TRUSTED_BUCKET}: {e}")
        
except Exception as e:
    print(f"MinIO error: {e}")

# %% [markdown]
# # 2) Retrieval helpers 
# 

# %%
_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Match Chroma's default OpenCLIPEmbeddingFunction() config
_MODEL_NAME = "ViT-B-32"
_PRETRAINED = "laion2b_s34b_b79k"

_CLIP_MODEL, _CLIP_PREPROCESS, _ = open_clip.create_model_and_transforms(
    _MODEL_NAME,
    pretrained=_PRETRAINED,
    device=_DEVICE
)

@torch.no_grad()
def encode_image_to_vec(pil_img: Image.Image) -> list[float]:
    """
    Produce an OpenCLIP image embedding compatible with the vectors stored in
    the 'trusted_zone_images' Chroma collection.
    """
    img_tensor = _CLIP_PREPROCESS(pil_img).unsqueeze(0).to(_DEVICE)
    img_features = _CLIP_MODEL.encode_image(img_tensor)
    img_features = img_features / img_features.norm(dim=-1, keepdim=True)
    return img_features.squeeze(0).cpu().tolist()

import io
import matplotlib.pyplot as plt

def show_retrieved_images(local_query_path, hits):
    """Show the query image followed by the retrieved similar images."""
    # --- display the query image ---
    plt.figure(figsize=(15, 4))
    plt.subplot(1, len(hits) + 1, 1)
    query_img = Image.open(local_query_path).convert("RGB")
    plt.imshow(query_img)
    plt.axis("off")
    plt.title("Query")

    # --- display each retrieved image from MinIO ---
    for i, hit in enumerate(hits, start=2):
        bucket = hit["image_s3_bucket"]
        key = hit["image_s3_key"]
        score = hit["score"]

        obj = s3.get_object(Bucket=bucket, Key=key)
        img = Image.open(io.BytesIO(obj["Body"].read())).convert("RGB")

        plt.subplot(1, len(hits) + 1, i)
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"{i-1}. Distance={score:.3f}", fontsize=8)

    plt.tight_layout()
    plt.show()

import textwrap

def show_text_results(result):
    print(f"🔎 Query: {result['query']}\n")
    for i, hit in enumerate(result["hits"], start=1):
        text_preview = textwrap.shorten(hit["text"], width=180, placeholder="…")
        print(f"{i}.  Distance={hit['score']:.3f}")
        print(text_preview)
        print("-" * 80)



# %%
ef_img = OpenCLIPEmbeddingFunction(model_name="ViT-B-32")
ef_text = SentenceTransformerEmbeddingFunction(model_name="Qwen/Qwen3-Embedding-0.6B")

def get_text_collection():
    return CHROMA.get_collection(
        name="trusted_zone_documents",
        embedding_function=ef_text,
    )

def get_image_collection():
    return CHROMA.get_collection(
        name="trusted_zone_images",
        embedding_function=ef_img,
    )

def retrieve_text(query: str, k: int = 5) -> Dict[str, Any]:
    """
    Retrieve top-k similar text docs from Chroma for a given natural language query.
    Returns a dict with 'hits' = [{text, meta, score}, ...]
    """
    col = get_text_collection()

    result = col.query(
        query_texts=[query],
        n_results=k,
        include=["documents", "metadatas", "distances"],
    )
    
    hits = []
    docs = result.get("documents", [[]])[0]
    metas = result.get("metadatas", [[]])[0]
    dists = result.get("distances", [[]])[0]

    for text_doc, meta, dist in zip(docs, metas, dists):
        hits.append({
            "text": text_doc,
            "meta": meta,
            "score": float(dist),
        })

    return {
        "query": query,
        "hits": hits,
    }

def retrieve_images(
    image_path: str,
    k: int = 5,
) -> Dict[str, Any]:
    """
    Retrieve top-k similar IMAGES from Chroma.
      image→image search:
         - image_path is provided
         - we take that local image, build its OpenCLIP IMAGE embedding
           using encode_image_to_vec(...)
         - then we pass that vector as query_embeddings=[...]
    """
    col = get_image_collection()

    pil_img = Image.open(image_path).convert("RGB")
    img_vec = encode_image_to_vec(pil_img)  # list[float]

    result = col.query(
        query_embeddings=[img_vec],
        n_results=k,
        include=["metadatas", "distances"],
    )

    metas = result.get("metadatas", [[]])[0]
    dists = result.get("distances", [[]])[0]

    hits = []
    for meta, dist in zip(metas, dists):
        hits.append({
            "image_s3_bucket": meta.get("bucket"),
            "image_s3_key": meta.get("object_key"),
            "score": float(dist),
        })

    return {"hits": hits}

# %%
# Debug: Check what collections exist
print("Available collections:")
for col_name in CHROMA.list_collections():
    print(f"  - {col_name.name}")
    
# Check the specific collection
try:
    col = CHROMA.get_collection("trusted_zone_documents")
    count = col.count()
    print(f"\nCollection 'trusted_zone_documents' has {count} documents")
    
    if count > 0:
        # Get a sample
        sample = col.get(limit=2)
        print("Sample documents:")
        for i, doc in enumerate(sample['documents'][:2]):
            print(f"[{i+1}] {doc[:200]}...")
except Exception as e:
    print(f"Error accessing collection: {e}")

# %% [markdown]
# # 3) Image to image search
# 

# %%
test_image_path = "fettuccine-alfredeo.jpg"  # example
result_images = retrieve_images(test_image_path, k=5)
result_images

hits = result_images["hits"]
for h in hits:
    print(h["image_s3_key"], "→", round(h["score"], 3))

show_retrieved_images(test_image_path, result_images["hits"])



# %%
test_image_path = "calico-beans.jpg"  # example
result_images = retrieve_images(test_image_path, k=5)
result_images

hits = result_images["hits"]
for h in hits:
    print(h["image_s3_key"], "→", round(h["score"], 3))

show_retrieved_images(test_image_path, result_images["hits"])



# %% [markdown]
# # 4) Text to text search
# 

# %%
query = "crunchy onion potato bake"
result_texts = retrieve_text(query, k=5)
show_text_results(result_texts)

# %%
query = "chicken noodle soup"
result_texts = retrieve_text(query, k=5)
show_text_results(result_texts)


