{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6436d2d4",
   "metadata": {},
   "source": [
    "# Trusted Zone — Documents Processing\n",
    "\n",
    "This notebook handles the **document processing** step for the Trusted Zone of our data pipeline.  \n",
    "Its primary goal is to:\n",
    "\n",
    "1. **Load recipe IDs** that have images (from the images processing step)\n",
    "2. **Stream-filter the recipe documents** to keep only those with images\n",
    "3. **Use multipart uploads** to handle large JSONL files efficiently\n",
    "4. **Generate a processing report** for audit and monitoring\n",
    "\n",
    "This notebook works in conjunction with `images.ipynb` to ensure data integrity in the Trusted Zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61cea2",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, re\n",
    "from pathlib import PurePosixPath\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Set, Iterable\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# S3 / MinIO Configuration\n",
    "MINIO_USER     = os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
    ")\n",
    "\n",
    "# Paths and Buckets\n",
    "FORM_BUCKET         = \"formatted-zone\"\n",
    "FORM_DOCS_KEY       = \"documents/recipes.jsonl\"\n",
    "\n",
    "TRUST_BUCKET        = \"trusted-zone\"\n",
    "TRUST_DOCS_KEY      = \"documents/recipes.jsonl\"\n",
    "TRUST_REPORT_PREFIX = \"reports\"\n",
    "\n",
    "# Input file from images processing\n",
    "RECIPE_IDS_FILE = \"recipe_ids_with_images.json\"\n",
    "\n",
    "# Behavior flags\n",
    "DRY_RUN   = False\n",
    "OVERWRITE = True\n",
    "\n",
    "def utc_ts():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Documents config & helpers ---\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Text cleaning config\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "LANG = \"english\"\n",
    "STOPWORDS = set(stopwords.words(LANG))\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = unidecode(s)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)         # remove punctuation\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    toks = [t for t in s.split() if t not in STOPWORDS]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def join_text_list(objs, key=\"text\"):\n",
    "    if not isinstance(objs, list):\n",
    "        return \"\"\n",
    "    return \" \".join([str(o.get(key, \"\")) for o in objs if isinstance(o, dict)])\n",
    "\n",
    "# Nutritional fields we will consider (adjust if your schema differs)\n",
    "# Case A (totals per 100g): dict under key like nutr_values_per100g__from_recipes_with_nutritional_info\n",
    "NUTR_TOTALS_KEY_CANDIDATES = [\n",
    "    \"nutr_values_per100g__from_recipes_with_nutritional_info\",\n",
    "    \"nutr_values_per100g\",\n",
    "    \"nutr_values\",\n",
    "]\n",
    "NUTR_TOTALS_NUMERIC_FIELDS = [\"energy\", \"fat\", \"protein\", \"salt\", \"saturates\", \"sugars\"]\n",
    "\n",
    "# Case B (per-ingredient): array of dicts under key like nutr_per_ingredient__from_recipes_with_nutritional_info\n",
    "NUTR_PER_INGR_KEY_CANDIDATES = [\n",
    "    \"nutr_per_ingredient__from_recipes_with_nutritional_info\",\n",
    "    \"nutr_per_ingredient\",\n",
    "]\n",
    "\n",
    "# Policy: if per-ingredient exists, we will drop totals to avoid duplication.\n",
    "DROP_TOTALS_IF_PER_INGR_PRESENT = True\n",
    "\n",
    "# Helper: pick first existing key\n",
    "def pick_first_present(rec: dict, candidates: list[str]) -> str | None:\n",
    "    for k in candidates:\n",
    "        if k in rec and rec[k] is not None:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "# IQR helper\n",
    "def iqr_bounds(vals: list[float], k: float = 1.5):\n",
    "    if not vals:\n",
    "        return None\n",
    "    s = pd.Series(vals)\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = float(q1 - k * iqr), float(q3 + k * iqr)\n",
    "    return {\"q1\": float(q1), \"q3\": float(q3), \"lo\": lo, \"hi\": hi}\n",
    "\n",
    "\n",
    "\n",
    "# Stats accumulator for the report\n",
    "doc_stats = {\n",
    "    \"total_read\": 0,\n",
    "    \"kept_after_id\": 0,\n",
    "    \"dropped_missing_nutrition\": 0,\n",
    "    \"dropped_outliers\": 0,\n",
    "    \"written\": 0,\n",
    "    \"nutrition_totals_dropped_due_to_duplication\": 0,\n",
    "    \"text_tokens_avg\": {\n",
    "        \"ingredients_raw\": 0.0, \"ingredients_clean\": 0.0,\n",
    "        \"instructions_raw\": 0.0, \"instructions_clean\": 0.0,\n",
    "        \"title_raw\": 0.0, \"title_clean\": 0.0,\n",
    "    }\n",
    "}\n",
    "\n",
    "def token_len(s: str) -> int:\n",
    "    return len((s or \"\").split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08061306",
   "metadata": {},
   "source": [
    "## 2. Load Recipe IDs from Images Processing\n",
    "\n",
    "We load the recipe IDs that have images, which were generated by the `images.ipynb` notebook. This creates the coupling between the two processing steps while maintaining clean separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recipe_ids_with_images() -> Set[str]:\n",
    "    \"\"\"Load recipe IDs that have images from the images processing step.\"\"\"\n",
    "    try:\n",
    "        with open(RECIPE_IDS_FILE, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        recipe_ids = set(data.get('recipe_ids_with_images', []))\n",
    "        print(f\"[INFO] Loaded {len(recipe_ids)} recipe IDs with images\")\n",
    "        print(f\"[INFO] Source: {data.get('source', 'unknown')}\")\n",
    "        print(f\"[INFO] Generated at: {data.get('timestamp', 'unknown')}\")\n",
    "        \n",
    "        return recipe_ids\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Recipe IDs file not found: {RECIPE_IDS_FILE}\")\n",
    "        print(\"[INFO] Make sure to run images.ipynb first to generate the recipe IDs\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load recipe IDs: {e}\")\n",
    "        return set()\n",
    "\n",
    "# Load the recipe IDs that have images\n",
    "recipe_ids_with_images = load_recipe_ids_with_images()\n",
    "\n",
    "if not recipe_ids_with_images:\n",
    "    print(\"[ERROR] No recipe IDs with images found. Cannot proceed with document filtering.\")\n",
    "    raise SystemExit(\"No recipe IDs with images found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-scanning documents to compute IQR thresholds for nutritional fields...\")\n",
    "\n",
    "def read_jsonl_lines(bucket: str, key: str):\n",
    "    \"\"\"Stream JSONL lines from S3.\"\"\"\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    for raw in obj[\"Body\"].iter_lines():\n",
    "        if raw:  # skip empty\n",
    "            yield raw\n",
    "\n",
    "nutr_values_collections = {f: [] for f in NUTR_TOTALS_NUMERIC_FIELDS}\n",
    "total_prescan = 0\n",
    "\n",
    "for raw in read_jsonl_lines(FORM_BUCKET, FORM_DOCS_KEY):\n",
    "    try:\n",
    "        rec = json.loads(raw)\n",
    "    except Exception:\n",
    "        continue\n",
    "    total_prescan += 1\n",
    "\n",
    "    # Only consider records we *might* keep later: ids_with_images\n",
    "    rid = rec.get(\"id\")\n",
    "    if not rid or rid not in recipe_ids_with_images:\n",
    "        continue\n",
    "\n",
    "    totals_key = pick_first_present(rec, NUTR_TOTALS_KEY_CANDIDATES)\n",
    "    if not totals_key:\n",
    "        continue\n",
    "    totals = rec.get(totals_key) or {}\n",
    "    # Collect numeric fields if present and finite\n",
    "    for f in NUTR_TOTALS_NUMERIC_FIELDS:\n",
    "        v = totals.get(f)\n",
    "        if isinstance(v, (int, float)) and math.isfinite(v):\n",
    "            nutr_values_collections[f].append(float(v))\n",
    "\n",
    "# Compute bounds\n",
    "iqr_thresholds = {}\n",
    "for f, vals in nutr_values_collections.items():\n",
    "    iqr_thresholds[f] = iqr_bounds(vals) if vals else None\n",
    "\n",
    "print(\"IQR thresholds:\", iqr_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaead4",
   "metadata": {},
   "source": [
    "## 3. Multipart JSONL Writer for Large Files\n",
    "\n",
    "The recipes dataset (`recipes.jsonl`) can be extremely large. Instead of loading everything into memory, we use a streaming approach with multipart uploads to handle the filtering efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PART_SIZE = 8 * 1024 * 1024  # 8 MB\n",
    "\n",
    "class MultipartJSONLWriter:\n",
    "    \"\"\"Streaming JSONL writer that uses S3 multipart uploads for large files.\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket: str, key: str, content_type=\"application/x-ndjson\", metadata=None):\n",
    "        self.bucket = bucket\n",
    "        self.key = key\n",
    "        self.buf = io.BytesIO()\n",
    "        self.parts = []\n",
    "        self.part_num = 1\n",
    "        self.open = True\n",
    "        extra = {\n",
    "            \"Bucket\": bucket,\n",
    "            \"Key\": key,\n",
    "            \"ContentType\": content_type,\n",
    "            \"Metadata\": metadata or {},\n",
    "        }\n",
    "        resp = s3.create_multipart_upload(**extra)\n",
    "        self.upload_id = resp[\"UploadId\"]\n",
    "\n",
    "    def _flush_part(self):\n",
    "        \"\"\"Flush the current buffer as a multipart upload part.\"\"\"\n",
    "        self.buf.seek(0)\n",
    "        body = self.buf.read()\n",
    "        if not body:\n",
    "            self.buf.seek(0)\n",
    "            self.buf.truncate(0)\n",
    "            return\n",
    "        resp = s3.upload_part(\n",
    "            Bucket=self.bucket, Key=self.key,\n",
    "            UploadId=self.upload_id, PartNumber=self.part_num, Body=body\n",
    "        )\n",
    "        self.parts.append({\"ETag\": resp[\"ETag\"], \"PartNumber\": self.part_num})\n",
    "        self.part_num += 1\n",
    "        self.buf.seek(0); self.buf.truncate(0)\n",
    "\n",
    "    def write_line(self, raw_line_bytes: bytes):\n",
    "        \"\"\"Write a line to the buffer, flushing when buffer is full.\"\"\"\n",
    "        # raw_line_bytes is already one JSON object line (no trailing \\n required)\n",
    "        self.buf.write(raw_line_bytes)\n",
    "        self.buf.write(b\"\\n\")\n",
    "        if self.buf.tell() >= MIN_PART_SIZE:\n",
    "            self._flush_part()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the writer and complete the multipart upload.\"\"\"\n",
    "        if not self.open:\n",
    "            return\n",
    "        try:\n",
    "            # If there's leftover data, flush as a last part\n",
    "            self._flush_part()\n",
    "            if not self.parts:\n",
    "                # No data kept: abort multipart, optionally create empty object\n",
    "                s3.abort_multipart_upload(\n",
    "                    Bucket=self.bucket, Key=self.key, UploadId=self.upload_id\n",
    "                )\n",
    "                # Optional: write a 0-byte file so the path exists\n",
    "                s3.put_object(\n",
    "                    Bucket=self.bucket, Key=self.key, Body=b\"\",\n",
    "                    ContentType=\"application/x-ndjson\",\n",
    "                    Metadata={\"note\": \"empty after filtering\", \"ts\": utc_ts()},\n",
    "                )\n",
    "            else:\n",
    "                s3.complete_multipart_upload(\n",
    "                    Bucket=self.bucket, Key=self.key, UploadId=self.upload_id,\n",
    "                    MultipartUpload={\"Parts\": self.parts}\n",
    "                )\n",
    "        finally:\n",
    "            self.open = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8d41d",
   "metadata": {},
   "source": [
    "## 4. Stream-Filter Documents by Recipe IDs\n",
    "\n",
    "Now we filter the recipe documents to keep only those that have corresponding images. This step ensures that every recipe in the Trusted Zone can be paired with one or more valid images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fcf332",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Stream-Filter + Transform to Trusted Zone\n",
    "\n",
    "def filter_docs_to_trusted_by_ids():\n",
    "    \"\"\"Keep only recipes with images; drop missing/outlier nutrition; clean text; drop duplicated totals.\"\"\"\n",
    "    total = kept_after_id = written = 0\n",
    "\n",
    "    if DRY_RUN:\n",
    "        print(\"[DRY_RUN] Counting documents that would be kept after id+nutrition checks...\")\n",
    "        for raw in read_jsonl_lines(FORM_BUCKET, FORM_DOCS_KEY):\n",
    "            total += 1\n",
    "            try:\n",
    "                rec = json.loads(raw)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            rid = rec.get(\"id\")\n",
    "            if not rid or rid not in recipe_ids_with_images:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Missing nutrition check (require at least totals OR per-ingredient)\n",
    "            totals_key = pick_first_present(rec, NUTR_TOTALS_KEY_CANDIDATES)\n",
    "            per_ingr_key = pick_first_present(rec, NUTR_PER_INGR_KEY_CANDIDATES)\n",
    "            if not totals_key and not per_ingr_key:\n",
    "                continue  # missing nutrition entirely\n",
    "\n",
    "            # Outlier check on totals if present and thresholds exist\n",
    "            if totals_key and iqr_thresholds and any(iqr_thresholds.values()):\n",
    "                totals = rec.get(totals_key) or {}\n",
    "                outlier = False\n",
    "                for f in NUTR_TOTALS_NUMERIC_FIELDS:\n",
    "                    thr = iqr_thresholds.get(f)\n",
    "                    v = totals.get(f)\n",
    "                    if thr and isinstance(v, (int, float)) and math.isfinite(v):\n",
    "                        if not (thr[\"lo\"] <= float(v) <= thr[\"hi\"]):\n",
    "                            outlier = True\n",
    "                            break\n",
    "                if outlier:\n",
    "                    continue\n",
    "\n",
    "            kept_after_id += 1\n",
    "\n",
    "        print(f\"[DRY_RUN] total={total} kept_after_id={kept_after_id}\")\n",
    "        return total, kept_after_id, 0\n",
    "\n",
    "    print(\"Filtering and transforming documents to Trusted Zone...\")\n",
    "\n",
    "    writer = MultipartJSONLWriter(\n",
    "        TRUST_BUCKET, TRUST_DOCS_KEY,\n",
    "        content_type=\"application/x-ndjson\",\n",
    "        metadata={\"note\": \"ids with images + nutrition checks + text clean + de-dup nutrition\", \"ts\": utc_ts()}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for raw in read_jsonl_lines(FORM_BUCKET, FORM_DOCS_KEY):\n",
    "            doc_stats[\"total_read\"] += 1\n",
    "            total += 1\n",
    "            try:\n",
    "                rec = json.loads(raw)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            rid = rec.get(\"id\")\n",
    "            if not rid or rid not in recipe_ids_with_images:\n",
    "                continue\n",
    "\n",
    "            # Require some nutrition info (either totals or per-ingredient)\n",
    "            # totals_key = pick_first_present(rec, NUTR_TOTALS_KEY_CANDIDATES)\n",
    "            # per_ingr_key = pick_first_present(rec, NUTR_PER_INGR_KEY_CANDIDATES)\n",
    "            # if not totals_key and not per_ingr_key:\n",
    "            #     doc_stats[\"dropped_missing_nutrition\"] += 1\n",
    "            #     continue\n",
    "\n",
    "            # Check nutrition info but don't drop recipes without it\n",
    "            totals_key = pick_first_present(rec, NUTR_TOTALS_KEY_CANDIDATES)\n",
    "            per_ingr_key = pick_first_present(rec, NUTR_PER_INGR_KEY_CANDIDATES)\n",
    "            has_nutrition = bool(totals_key or per_ingr_key)\n",
    "            rec[\"has_nutrition_data\"] = has_nutrition\n",
    "\n",
    "            # If totals exist, apply outlier filter per field\n",
    "            outlier = False\n",
    "            if totals_key and iqr_thresholds and any(iqr_thresholds.values()):\n",
    "                totals = rec.get(totals_key) or {}\n",
    "                for f in NUTR_TOTALS_NUMERIC_FIELDS:\n",
    "                    thr = iqr_thresholds.get(f)\n",
    "                    v = totals.get(f)\n",
    "                    if thr and isinstance(v, (int, float)) and math.isfinite(v):\n",
    "                        val = float(v)\n",
    "                        if not (thr[\"lo\"] <= val <= thr[\"hi\"]):\n",
    "                            outlier = True\n",
    "                            break\n",
    "            if outlier:\n",
    "                doc_stats[\"dropped_outliers\"] += 1\n",
    "                continue\n",
    "\n",
    "            doc_stats[\"kept_after_id\"] += 1\n",
    "\n",
    "            # Text cleaning for embeddings — add cleaned siblings without overwriting\n",
    "            title_raw = rec.get(\"title__from_layer1\") or rec.get(\"title__from_recipes_with_nutritional_info\") or rec.get(\"title\") or \"\"\n",
    "            ingr_raw  = join_text_list(rec.get(\"ingredients__from_layer1\")) or join_text_list(rec.get(\"ingredients__from_recipes_with_nutritional_info\")) or join_text_list(rec.get(\"ingredients\")) or \"\"\n",
    "            instr_raw = join_text_list(rec.get(\"instructions__from_layer1\")) or join_text_list(rec.get(\"instructions__from_recipes_with_nutritional_info\")) or join_text_list(rec.get(\"instructions\")) or \"\"\n",
    "\n",
    "            rec[\"title_text_raw\"] = title_raw\n",
    "            rec[\"ingredients_text_raw\"] = ingr_raw\n",
    "            rec[\"instructions_text_raw\"] = instr_raw\n",
    "\n",
    "            rec[\"title_text_clean\"] = clean_text(title_raw)\n",
    "            rec[\"ingredients_text_clean\"] = clean_text(ingr_raw)\n",
    "            rec[\"instructions_text_clean\"] = clean_text(instr_raw)\n",
    "\n",
    "            # Update running token-length averages (simple incremental mean)\n",
    "            for k, v in {\n",
    "                \"ingredients_raw\": token_len(rec[\"ingredients_text_raw\"]),\n",
    "                \"ingredients_clean\": token_len(rec[\"ingredients_text_clean\"]),\n",
    "                \"instructions_raw\": token_len(rec[\"instructions_text_raw\"]),\n",
    "                \"instructions_clean\": token_len(rec[\"instructions_text_clean\"]),\n",
    "                \"title_raw\": token_len(rec[\"title_text_raw\"]),\n",
    "                \"title_clean\": token_len(rec[\"title_text_clean\"]),\n",
    "            }.items():\n",
    "                # incremental average\n",
    "                n = max(1, doc_stats[\"kept_after_id\"])\n",
    "                prev = doc_stats[\"text_tokens_avg\"][k]\n",
    "                doc_stats[\"text_tokens_avg\"][k] = prev + (v - prev) / n\n",
    "\n",
    "            # Remove duplicated totals if per-ingredient available (policy)\n",
    "            if DROP_TOTALS_IF_PER_INGR_PRESENT and per_ingr_key and totals_key:\n",
    "                if totals_key in rec:\n",
    "                    rec.pop(totals_key, None)\n",
    "                    doc_stats[\"nutrition_totals_dropped_due_to_duplication\"] += 1\n",
    "                rec[\"nutrition_normalized\"] = True\n",
    "            else:\n",
    "                rec[\"nutrition_normalized\"] = bool(per_ingr_key and not totals_key)\n",
    "\n",
    "            # Write transformed record\n",
    "            writer.write_line(json.dumps(rec, ensure_ascii=False).encode(\"utf-8\"))\n",
    "            written += 1\n",
    "            doc_stats[\"written\"] += 1\n",
    "\n",
    "    finally:\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"[OK] wrote filtered+transformed docs to s3://{TRUST_BUCKET}/{TRUST_DOCS_KEY}\")\n",
    "    return total, doc_stats[\"kept_after_id\"], written\n",
    "\n",
    "# Execute the filtering\n",
    "total_docs, kept_docs, written_docs = filter_docs_to_trusted_by_ids()\n",
    "print(f\"[STATS] total_read={total_docs} kept_after_id={kept_docs} written={written_docs}\")\n",
    "print(f\"[STATS] dropped_missing_nutrition={doc_stats['dropped_missing_nutrition']} dropped_outliers={doc_stats['dropped_outliers']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f37c2",
   "metadata": {},
   "source": [
    "## 5. Generate Processing Report\n",
    "\n",
    "Finally, we generate a comprehensive report of the document processing step and save it to the Trusted Zone for audit and monitoring purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Generate Processing Report (expanded)\n",
    "\n",
    "report = {\n",
    "    \"timestamp\": utc_ts(),\n",
    "    \"processing_step\": \"documents\",\n",
    "    \"source_docs\": f\"s3://{FORM_BUCKET}/{FORM_DOCS_KEY}\",\n",
    "    \"destination_docs\": f\"s3://{TRUST_BUCKET}/{TRUST_DOCS_KEY}\",\n",
    "    \"recipe_ids_source\": RECIPE_IDS_FILE,\n",
    "\n",
    "    # Counts\n",
    "    \"total_doc_count\": total_docs,\n",
    "    \"kept_after_id\": kept_docs,\n",
    "    \"written_doc_count\": written_docs,\n",
    "    \"dropped_missing_nutrition\": doc_stats[\"dropped_missing_nutrition\"],\n",
    "    \"dropped_outliers\": doc_stats[\"dropped_outliers\"],\n",
    "    \"unique_recipe_ids_with_images\": len(recipe_ids_with_images),\n",
    "    \"filtering_rate\": f\"{(written_docs/total_docs*100):.2f}%\" if total_docs > 0 else \"0%\",\n",
    "\n",
    "\n",
    "    # Nutrition policy & thresholds\n",
    "    \"nutrition\": {\n",
    "        \"totals_key_candidates\": NUTR_TOTALS_KEY_CANDIDATES,\n",
    "        \"per_ingredient_key_candidates\": NUTR_PER_INGR_KEY_CANDIDATES,\n",
    "        \"drop_totals_if_per_ingredient_present\": DROP_TOTALS_IF_PER_INGR_PRESENT,\n",
    "        \"iqr_thresholds\": iqr_thresholds\n",
    "    },\n",
    "\n",
    "    # Text cleaning summary\n",
    "    \"text_cleaning\": {\n",
    "        \"lang\": LANG,\n",
    "        \"stopwords_count\": len(STOPWORDS),\n",
    "        \"token_len_avg\": doc_stats[\"text_tokens_avg\"]\n",
    "    },\n",
    "\n",
    "    # Effects\n",
    "    \"nutrition_totals_dropped_due_to_duplication\": doc_stats[\"nutrition_totals_dropped_due_to_duplication\"],\n",
    "\n",
    "    # Flags\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"overwrite\": OVERWRITE\n",
    "}\n",
    "\n",
    "if not DRY_RUN:\n",
    "    s3.put_object(\n",
    "        Bucket=TRUST_BUCKET,\n",
    "        Key=f\"{TRUST_REPORT_PREFIX}/documents_processing_{utc_ts()}.json\",\n",
    "        Body=json.dumps(report, ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "    print(f\"[OK] wrote report -> s3://{TRUST_BUCKET}/{TRUST_REPORT_PREFIX}/\")\n",
    "else:\n",
    "    print(\"[DRY_RUN] report:\", json.dumps(report, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
