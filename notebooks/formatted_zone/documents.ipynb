{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3ef588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, re, json\n",
    "from decimal import Decimal\n",
    "from pathlib import PurePosixPath\n",
    "import boto3, ijson\n",
    "from dotenv import load_dotenv\n",
    "from botocore.config import Config\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DOC_SRC_BUCKET =  \"landing-zone\"\n",
    "DOC_SRC_PREFIX = \"persistent_landing/documents\"\n",
    "OUT_S3_BUCKET  = \"formatted-zone\"\n",
    "OUT_S3_KEY     = \"documents/recipes.jsonl\"\n",
    "\n",
    "SKIP_FIELDS = {\"url\", \"partition\"}\n",
    "\n",
    "DOC_EXTS = (\".json\", \".jsonl\", \".ndjson\")\n",
    "ALWAYS_TAG = True\n",
    "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT =os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a19965",
   "metadata": {},
   "source": [
    "This block loads environment variables and sets up MinIO (via **boto3**) to read raw documents and write a formatted output. It defines source and destination S3 paths, basic filters (file extensions to read and fields to skip), and a flag for tagging. Then it creates a boto3 session/client pointed at the MinIO endpoint using credentials from `.env`, so later steps can stream JSON/JSONL inputs from `landing-zone` and write the consolidated result to `formatted-zone/recipes.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e45015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_builtin(x):\n",
    "    if isinstance(x, Decimal):\n",
    "        return int(x) if x == x.to_integral_value() else float(x)\n",
    "    if isinstance(x, dict):  return {k: to_builtin(v) for k, v in x.items()}\n",
    "    if isinstance(x, list):  return [to_builtin(v) for v in x]\n",
    "    return x\n",
    "\n",
    "\n",
    "def list_docs(bucket: str, prefix: str):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\"/\"):\n",
    "                continue\n",
    "            if key.lower().endswith(DOC_EXTS):\n",
    "                yield key\n",
    "\n",
    "\n",
    "def iter_jsonl(bucket, key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    for line in obj[\"Body\"].iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        yield to_builtin(json.loads(line))\n",
    "\n",
    "\n",
    "def iter_json_array(bucket, key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    for item in ijson.items(obj[\"Body\"], \"item\"):\n",
    "        yield to_builtin(item)\n",
    "\n",
    "\n",
    "def detect_iter(bucket, key):\n",
    "    if key.lower().endswith((\".jsonl\", \".ndjson\")):\n",
    "        return iter_jsonl(bucket, key)\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    head = obj[\"Body\"].read(2048)\n",
    "    obj[\"Body\"].close()\n",
    "    if head.lstrip()[:1] == b\"[\":\n",
    "        return iter_json_array(bucket, key)\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = obj[\"Body\"].read()\n",
    "    try:\n",
    "        one = to_builtin(json.loads(body))\n",
    "        def _once():\n",
    "            if isinstance(one, dict):\n",
    "                yield one\n",
    "        return _once()\n",
    "    except Exception:\n",
    "        return iter(())\n",
    "\n",
    "\n",
    "def short_tag_from_key(key: str) -> str:\n",
    "    stem = PurePosixPath(key).stem\n",
    "    if \"__\" in stem:\n",
    "        cand = stem.split(\"__\")[-1]\n",
    "    else:\n",
    "        toks = re.split(r\"[-_$]+\", stem)\n",
    "        cand = toks[-1] if toks else stem\n",
    "    tag = re.sub(r\"[^\\w\\-]+\", \"_\", cand).strip(\"_\").lower()\n",
    "    return (tag[-40:] if len(tag) > 40 else tag) or \"file\"\n",
    "\n",
    "\n",
    "def merge_with_tags(dst: dict, src: dict, tag: str):\n",
    "    if not isinstance(src, dict):\n",
    "        return\n",
    "    for k, v in src.items():\n",
    "        if k in SKIP_FIELDS:\n",
    "            continue\n",
    "        if k == \"id\":\n",
    "            if \"id\" not in dst:\n",
    "                dst[\"id\"] = v\n",
    "            continue\n",
    "        base = f\"{k}__from_{tag}\" if ALWAYS_TAG else k\n",
    "        new_k = base\n",
    "        i = 2\n",
    "        while new_k in dst:\n",
    "            if not ALWAYS_TAG and new_k == k:\n",
    "                base = f\"{k}__from_{tag}\"\n",
    "                new_k = base\n",
    "                continue\n",
    "            new_k = f\"{base}_{i}\"\n",
    "            i += 1\n",
    "        dst[new_k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06992653",
   "metadata": {},
   "source": [
    "This block provides tools for reading and merging JSON documents stored in MinIO.\n",
    "\n",
    "The conversion function `to_builtin` standardizes data types, turning objects like `Decimal` into basic Python numbers. The iterators (`list_docs`, `iter_jsonl`, `iter_json_array`, and `detect_iter`) automatically detect the file format—JSON, JSONL, or NDJSON—and stream records efficiently, allowing the script to handle large datasets without loading everything into memory.\n",
    "\n",
    "`short_tag_from_key` extracts a short, clean tag from each file name, used to identify the source of data when merging. Finally, `merge_with_tags` combines multiple JSON records into one, renaming fields with a tag suffix to avoid collisions and preserve the origin of each field. This ensures consistent, traceable merging across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da295b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 3 document file(s) under s3://landing-zone/persistent_landing/documents\n",
      "[MERGE] (1/3) persistent_landing/documents/document$adsdb-multimodal-food-data-management$2025-10-11T17-15-55Z$803760260afbeee8__det_ingrs.json [tag=det_ingrs] -> records processed: 1029720\n",
      "[MERGE] (2/3) persistent_landing/documents/document$adsdb-multimodal-food-data-management$2025-10-11T17-15-55Z$a0ad8838a448b589__recipes_with_nutritional_info.json [tag=recipes_with_nutritional_info] -> records processed: 51235\n",
      "[MERGE] (3/3) persistent_landing/documents/document$adsdb-multimodal-food-data-management$2025-10-11T17-15-55Z$bdcc01219cf0d860__layer1.json [tag=layer1] -> records processed: 1029720\n",
      "[STATS] Total records processed: 2110675\n",
      "[STATS] Unique ids merged:       1029720\n",
      "[OK] Wrote JSONL sample to s3://formatted-zone/documents/recipes.jsonl\n"
     ]
    }
   ],
   "source": [
    "keys = list(list_docs(DOC_SRC_BUCKET, DOC_SRC_PREFIX))\n",
    "if not keys:\n",
    "    raise RuntimeError(f\"No documents under s3://{DOC_SRC_BUCKET}/{DOC_SRC_PREFIX}\")\n",
    "print(f\"[INFO] Found {len(keys)} document file(s) under s3://{DOC_SRC_BUCKET}/{DOC_SRC_PREFIX}\")\n",
    "\n",
    "joined: dict[str, dict] = {}\n",
    "total_seen = 0\n",
    "\n",
    "for idx, key in enumerate(keys, 1):\n",
    "    tag = short_tag_from_key(key)\n",
    "    relevant = 0\n",
    "    for rec in detect_iter(DOC_SRC_BUCKET, key):\n",
    "        rid = rec.get(\"id\")\n",
    "        if not rid:\n",
    "            continue\n",
    "        if rid not in joined:\n",
    "            joined[rid] = {}\n",
    "        merge_with_tags(joined[rid], rec, tag)\n",
    "        relevant += 1\n",
    "        total_seen += 1\n",
    "    print(f\"[MERGE] ({idx}/{len(keys)}) {key} [tag={tag}] -> records processed: {relevant}\")\n",
    "\n",
    "print(f\"[STATS] Total records processed: {total_seen}\")\n",
    "print(f\"[STATS] Unique ids merged:       {len(joined)}\")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for rid in sorted(joined.keys()):\n",
    "    buf.write(json.dumps(joined[rid], ensure_ascii=False) + \"\\n\")\n",
    "payload = buf.getvalue().encode(\"utf-8\")\n",
    "\n",
    "s3.put_object(\n",
    "    Bucket=OUT_S3_BUCKET,\n",
    "    Key=OUT_S3_KEY,\n",
    "    Body=payload,\n",
    "    ContentType=\"application/x-ndjson\",\n",
    "    Metadata={\"source-prefix\": f\"s3://{DOC_SRC_BUCKET}/{DOC_SRC_PREFIX}\",\n",
    "                \"note\": \"sample join by id with tagged collisions; plain ndjson\"},\n",
    ")\n",
    "print(f\"[OK] Wrote JSONL sample to s3://{OUT_S3_BUCKET}/{OUT_S3_KEY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15f32c",
   "metadata": {},
   "source": [
    "This block gathers all document keys under the source prefix, fails fast if none are found, and then streams each file to merge records by `id`. For every file, it derives a short source tag and uses it to rename colliding fields, preserving provenance. The result is a dictionary keyed by `id` with tagged fields from all inputs.\n",
    "\n",
    "Finally, it serializes the merged data as **NDJSON** and writes it to `formatted-zone/documents/recipes.jsonl` with metadata about the source prefix. The NDJSON format is used because it is more compact than regular JSON, reducing storage size and improving processing efficiency when handling large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
