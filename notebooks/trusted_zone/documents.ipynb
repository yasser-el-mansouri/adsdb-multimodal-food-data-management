{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6436d2d4",
   "metadata": {},
   "source": [
    "# Trusted Zone â€” Documents Processing\n",
    "\n",
    "This notebook handles the **document processing** step for the Trusted Zone of our data pipeline.  \n",
    "Its primary goal is to:\n",
    "\n",
    "1. **Load recipe IDs** that have images (from the images processing step)\n",
    "2. **Stream-filter the recipe documents** to keep only those with images\n",
    "3. **Use multipart uploads** to handle large JSONL files efficiently\n",
    "4. **Generate a processing report** for audit and monitoring\n",
    "\n",
    "This notebook works in conjunction with `images.ipynb` to ensure data integrity in the Trusted Zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61cea2",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbbd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, re\n",
    "from pathlib import PurePosixPath\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Set, Iterable\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# S3 / MinIO Configuration\n",
    "MINIO_USER     = os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
    ")\n",
    "\n",
    "# Paths and Buckets\n",
    "FORM_BUCKET         = \"formatted-zone\"\n",
    "FORM_DOCS_KEY       = \"documents/recipes.jsonl\"\n",
    "\n",
    "TRUST_BUCKET        = \"trusted-zone\"\n",
    "TRUST_DOCS_KEY      = \"documents/recipes.jsonl\"\n",
    "TRUST_REPORT_PREFIX = \"reports\"\n",
    "\n",
    "# Input file from images processing\n",
    "RECIPE_IDS_FILE = \"recipe_ids_with_images.json\"\n",
    "\n",
    "# Behavior flags\n",
    "DRY_RUN   = False\n",
    "OVERWRITE = True\n",
    "\n",
    "def utc_ts():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08061306",
   "metadata": {},
   "source": [
    "## 2. Load Recipe IDs from Images Processing\n",
    "\n",
    "We load the recipe IDs that have images, which were generated by the `images.ipynb` notebook. This creates the coupling between the two processing steps while maintaining clean separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec54cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 7 recipe IDs with images\n",
      "[INFO] Source: s3://formatted-zone/images/\n",
      "[INFO] Generated at: 2025-10-12T15-17-27Z\n"
     ]
    }
   ],
   "source": [
    "def load_recipe_ids_with_images() -> Set[str]:\n",
    "    \"\"\"Load recipe IDs that have images from the images processing step.\"\"\"\n",
    "    try:\n",
    "        with open(RECIPE_IDS_FILE, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        recipe_ids = set(data.get('recipe_ids_with_images', []))\n",
    "        print(f\"[INFO] Loaded {len(recipe_ids)} recipe IDs with images\")\n",
    "        print(f\"[INFO] Source: {data.get('source', 'unknown')}\")\n",
    "        print(f\"[INFO] Generated at: {data.get('timestamp', 'unknown')}\")\n",
    "        \n",
    "        return recipe_ids\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Recipe IDs file not found: {RECIPE_IDS_FILE}\")\n",
    "        print(\"[INFO] Make sure to run images.ipynb first to generate the recipe IDs\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load recipe IDs: {e}\")\n",
    "        return set()\n",
    "\n",
    "# Load the recipe IDs that have images\n",
    "recipe_ids_with_images = load_recipe_ids_with_images()\n",
    "\n",
    "if not recipe_ids_with_images:\n",
    "    print(\"[ERROR] No recipe IDs with images found. Cannot proceed with document filtering.\")\n",
    "    raise SystemExit(\"No recipe IDs with images found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaead4",
   "metadata": {},
   "source": [
    "## 3. Multipart JSONL Writer for Large Files\n",
    "\n",
    "The recipes dataset (`recipes.jsonl`) can be extremely large. Instead of loading everything into memory, we use a streaming approach with multipart uploads to handle the filtering efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcad2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PART_SIZE = 8 * 1024 * 1024  # 8 MB\n",
    "\n",
    "class MultipartJSONLWriter:\n",
    "    \"\"\"Streaming JSONL writer that uses S3 multipart uploads for large files.\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket: str, key: str, content_type=\"application/x-ndjson\", metadata=None):\n",
    "        self.bucket = bucket\n",
    "        self.key = key\n",
    "        self.buf = io.BytesIO()\n",
    "        self.parts = []\n",
    "        self.part_num = 1\n",
    "        self.open = True\n",
    "        extra = {\n",
    "            \"Bucket\": bucket,\n",
    "            \"Key\": key,\n",
    "            \"ContentType\": content_type,\n",
    "            \"Metadata\": metadata or {},\n",
    "        }\n",
    "        resp = s3.create_multipart_upload(**extra)\n",
    "        self.upload_id = resp[\"UploadId\"]\n",
    "\n",
    "    def _flush_part(self):\n",
    "        \"\"\"Flush the current buffer as a multipart upload part.\"\"\"\n",
    "        self.buf.seek(0)\n",
    "        body = self.buf.read()\n",
    "        if not body:\n",
    "            self.buf.seek(0)\n",
    "            self.buf.truncate(0)\n",
    "            return\n",
    "        resp = s3.upload_part(\n",
    "            Bucket=self.bucket, Key=self.key,\n",
    "            UploadId=self.upload_id, PartNumber=self.part_num, Body=body\n",
    "        )\n",
    "        self.parts.append({\"ETag\": resp[\"ETag\"], \"PartNumber\": self.part_num})\n",
    "        self.part_num += 1\n",
    "        self.buf.seek(0); self.buf.truncate(0)\n",
    "\n",
    "    def write_line(self, raw_line_bytes: bytes):\n",
    "        \"\"\"Write a line to the buffer, flushing when buffer is full.\"\"\"\n",
    "        # raw_line_bytes is already one JSON object line (no trailing \\n required)\n",
    "        self.buf.write(raw_line_bytes)\n",
    "        self.buf.write(b\"\\n\")\n",
    "        if self.buf.tell() >= MIN_PART_SIZE:\n",
    "            self._flush_part()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the writer and complete the multipart upload.\"\"\"\n",
    "        if not self.open:\n",
    "            return\n",
    "        try:\n",
    "            # If there's leftover data, flush as a last part\n",
    "            self._flush_part()\n",
    "            if not self.parts:\n",
    "                # No data kept: abort multipart, optionally create empty object\n",
    "                s3.abort_multipart_upload(\n",
    "                    Bucket=self.bucket, Key=self.key, UploadId=self.upload_id\n",
    "                )\n",
    "                # Optional: write a 0-byte file so the path exists\n",
    "                s3.put_object(\n",
    "                    Bucket=self.bucket, Key=self.key, Body=b\"\",\n",
    "                    ContentType=\"application/x-ndjson\",\n",
    "                    Metadata={\"note\": \"empty after filtering\", \"ts\": utc_ts()},\n",
    "                )\n",
    "            else:\n",
    "                s3.complete_multipart_upload(\n",
    "                    Bucket=self.bucket, Key=self.key, UploadId=self.upload_id,\n",
    "                    MultipartUpload={\"Parts\": self.parts}\n",
    "                )\n",
    "        finally:\n",
    "            self.open = False\n",
    "\n",
    "def read_jsonl_lines(bucket: str, key: str):\n",
    "    \"\"\"Stream JSONL lines from S3.\"\"\"\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    for raw in obj[\"Body\"].iter_lines():\n",
    "        if raw:  # skip empty\n",
    "            yield raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8d41d",
   "metadata": {},
   "source": [
    "## 4. Stream-Filter Documents by Recipe IDs\n",
    "\n",
    "Now we filter the recipe documents to keep only those that have corresponding images. This step ensures that every recipe in the Trusted Zone can be paired with one or more valid images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fcf332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering documents and uploading to Trusted Zone...\n",
      "[OK] wrote filtered docs to s3://trusted-zone/documents/recipes.jsonl\n",
      "[STATS] docs total=1029720 kept=7 dropped=1029713\n"
     ]
    }
   ],
   "source": [
    "def filter_docs_to_trusted_by_ids():\n",
    "    \"\"\"Filter documents to keep only those with corresponding images.\"\"\"\n",
    "    total = kept = 0\n",
    "    \n",
    "    if DRY_RUN:\n",
    "        print(\"[DRY_RUN] Counting documents that would be kept...\")\n",
    "        for raw in read_jsonl_lines(FORM_BUCKET, FORM_DOCS_KEY):\n",
    "            total += 1\n",
    "            try:\n",
    "                rid = json.loads(raw).get(\"id\")\n",
    "            except Exception:\n",
    "                continue\n",
    "            if rid in recipe_ids_with_images:\n",
    "                kept += 1\n",
    "        print(f\"[DRY_RUN] total={total} kept={kept}\")\n",
    "        return total, kept\n",
    "\n",
    "    print(\"Filtering documents and uploading to Trusted Zone...\")\n",
    "    \n",
    "    writer = MultipartJSONLWriter(\n",
    "        TRUST_BUCKET, TRUST_DOCS_KEY,\n",
    "        content_type=\"application/x-ndjson\",\n",
    "        metadata={\"note\": \"filtered to ids that have images\", \"ts\": utc_ts()}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for raw in read_jsonl_lines(FORM_BUCKET, FORM_DOCS_KEY):\n",
    "            total += 1\n",
    "            try:\n",
    "                rec = json.loads(raw)\n",
    "            except Exception:\n",
    "                continue\n",
    "            rid = rec.get(\"id\")\n",
    "            if rid and rid in recipe_ids_with_images:\n",
    "                kept += 1\n",
    "                writer.write_line(raw)\n",
    "    finally:\n",
    "        # Always close; it handles zero-kept gracefully\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"[OK] wrote filtered docs to s3://{TRUST_BUCKET}/{TRUST_DOCS_KEY}\")\n",
    "    return total, kept\n",
    "\n",
    "# Execute the filtering\n",
    "total_docs, kept_docs = filter_docs_to_trusted_by_ids()\n",
    "print(f\"[STATS] docs total={total_docs} kept={kept_docs} dropped={total_docs-kept_docs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f37c2",
   "metadata": {},
   "source": [
    "## 5. Generate Processing Report\n",
    "\n",
    "Finally, we generate a comprehensive report of the document processing step and save it to the Trusted Zone for audit and monitoring purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a749010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote report -> s3://trusted-zone/reports/\n",
      "\n",
      "============================================================\n",
      "DOCUMENTS PROCESSING COMPLETE\n",
      "============================================================\n",
      "Filtered 7 documents from 1,029,720 total\n",
      "Filtering rate: 0.00%\n",
      "Trusted Zone ready for analysis and modeling\n"
     ]
    }
   ],
   "source": [
    "report = {\n",
    "    \"timestamp\": utc_ts(),\n",
    "    \"processing_step\": \"documents\",\n",
    "    \"source_docs\": f\"s3://{FORM_BUCKET}/{FORM_DOCS_KEY}\",\n",
    "    \"destination_docs\": f\"s3://{TRUST_BUCKET}/{TRUST_DOCS_KEY}\",\n",
    "    \"recipe_ids_source\": RECIPE_IDS_FILE,\n",
    "    \"total_doc_count\": total_docs,\n",
    "    \"kept_doc_count\": kept_docs,\n",
    "    \"dropped_doc_count\": total_docs - kept_docs,\n",
    "    \"unique_recipe_ids_with_images\": len(recipe_ids_with_images),\n",
    "    \"filtering_rate\": f\"{(kept_docs/total_docs*100):.2f}%\" if total_docs > 0 else \"0%\",\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"overwrite\": OVERWRITE\n",
    "}\n",
    "\n",
    "if not DRY_RUN:\n",
    "    s3.put_object(\n",
    "        Bucket=TRUST_BUCKET,\n",
    "        Key=f\"{TRUST_REPORT_PREFIX}/documents_processing_{utc_ts()}.json\",\n",
    "        Body=json.dumps(report, ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "    print(f\"[OK] wrote report -> s3://{TRUST_BUCKET}/{TRUST_REPORT_PREFIX}/\")\n",
    "else:\n",
    "    print(\"[DRY_RUN] report:\", json.dumps(report, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOCUMENTS PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Filtered {kept_docs:,} documents from {total_docs:,} total\")\n",
    "print(f\"Filtering rate: {report['filtering_rate']}\")\n",
    "print(f\"Trusted Zone ready for analysis and modeling\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
