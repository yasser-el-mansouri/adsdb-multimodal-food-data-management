{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe9fdd7",
   "metadata": {},
   "source": [
    "# Trusted Zone — Images and Documents Filtering\n",
    "\n",
    "This notebook implements the transformation and validation step for the **Trusted Zone** of our data pipeline.  \n",
    "Its goal is to ensure that every recipe document kept in the Trusted Zone has at least one associated image, and that no orphan images are stored without a corresponding recipe entry.  \n",
    "\n",
    "By doing so, we maintain the integrity of the multimodal dataset and make the data ready for subsequent analytical and modeling stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43482ad",
   "metadata": {},
   "source": [
    "## 1. Setup and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f6328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, re\n",
    "from pathlib import PurePosixPath\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Set, Iterable\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# S3 / MinIO\n",
    "MINIO_USER     = os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
    ")\n",
    "\n",
    "# Paths\n",
    "FORM_BUCKET         = \"formatted-zone\"\n",
    "FORM_IMAGES_PREFIX  = \"images\"\n",
    "FORM_DOCS_KEY       = \"documents/recipes.jsonl\"\n",
    "\n",
    "TRUST_BUCKET        = \"trusted-zone\"\n",
    "TRUST_IMAGES_PREFIX = \"images\"\n",
    "TRUST_DOCS_KEY      = \"documents/recipes.jsonl\"\n",
    "TRUST_REPORT_PREFIX = \"reports\"\n",
    "\n",
    "# Behavior\n",
    "DRY_RUN   = False\n",
    "OVERWRITE = True\n",
    "\n",
    "def utc_ts():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899bfd5",
   "metadata": {},
   "source": [
    "## 2. S3 Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ee608",
   "metadata": {},
   "source": [
    "We begin by loading credentials and defining S3 client sessions (using the MinIO-compatible interface).  \n",
    "The Trusted Zone will receive its data from the **Formatted Zone**, which contains cleaned and standardized files.  \n",
    "These sections simply prepare the environment for accessing and writing data across the two buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fff9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_list_keys(bucket: str, prefix: str) -> Iterable[str]:\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []) or []:\n",
    "            key = obj[\"Key\"]\n",
    "            if not key.endswith(\"/\"):\n",
    "                yield key\n",
    "\n",
    "def s3_head(bucket: str, key: str):\n",
    "    try:\n",
    "        return s3.head_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        if e.response.get(\"Error\", {}).get(\"Code\") in (\"404\", \"NoSuchKey\", \"NotFound\"):\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "def s3_copy_object(src_bucket: str, src_key: str, dst_bucket: str, dst_key: str, overwrite: bool = True):\n",
    "    if not overwrite and s3_head(dst_bucket, dst_key) is not None:\n",
    "        return \"skip-exists\"\n",
    "    return s3.copy_object(\n",
    "        Bucket=dst_bucket,\n",
    "        Key=dst_key,\n",
    "        CopySource={\"Bucket\": src_bucket, \"Key\": src_key},\n",
    "        MetadataDirective=\"COPY\"\n",
    "    )\n",
    "\n",
    "def read_jsonl_lines(bucket: str, key: str):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    for raw in obj[\"Body\"].iter_lines():\n",
    "        if raw:  # skip empty\n",
    "            yield raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dc298",
   "metadata": {},
   "source": [
    "## 3. Extract recipeId from image filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111b595",
   "metadata": {},
   "source": [
    "Each image stored in the Formatted Zone follows a structured naming convention that encodes metadata, including the recipe identifier.  \n",
    "The general pattern is:\n",
    "\n",
    "**fileType$dataSource$ingestionTimestamp$hash__recipeId_positionOnImagesUrlArrayFromLayer2.extension**\n",
    "\n",
    "From these filenames, we extract the `recipeId` using a regular expression.  \n",
    "This allows us to associate every image with its corresponding recipe entry, even when multiple images exist for the same recipe.  \n",
    "The result of this step is two structures:\n",
    "\n",
    "- `img_ids`: a set of **unique recipe IDs** that have at least one image.\n",
    "- `id_to_imgkeys`: a dictionary mapping each `recipeId` to **all its image keys** (to preserve one-to-many relationships).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2155483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] scanned image keys: 11\n",
      "[INFO] unique recipeIds with images: 7\n",
      "[INFO] total image files matched to recipeIds: 11\n"
     ]
    }
   ],
   "source": [
    "# Recognize names like:\n",
    "#   images/type$src$ts$hash__000018c8a5_0.jpg\n",
    "#   images/type$src$ts$hash__abcd_ef-12_3.JPEG\n",
    "# ID part: letters, digits, underscore, dash\n",
    "ID_REGEX = re.compile(\n",
    "    r\"__([A-Za-z0-9_\\-]+)_(\\d+)\\.(?:jpe?g|png|webp|gif|bmp|tiff)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def recipe_id_from_image_key(key: str) -> str | None:\n",
    "    name = PurePosixPath(key).name\n",
    "    m = ID_REGEX.search(name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "img_ids: Set[str] = set()                    # unique IDs (filter purpose only)\n",
    "id_to_imgkeys: Dict[str, List[str]] = {}     # ALL images per ID\n",
    "\n",
    "count_keys = 0\n",
    "for key in s3_list_keys(FORM_BUCKET, FORM_IMAGES_PREFIX + \"/\"):\n",
    "    count_keys += 1\n",
    "    rid = recipe_id_from_image_key(key)\n",
    "    if not rid:\n",
    "        continue\n",
    "    img_ids.add(rid)\n",
    "    id_to_imgkeys.setdefault(rid, []).append(key)\n",
    "\n",
    "# Make copies deterministic (optional)\n",
    "for rid in id_to_imgkeys:\n",
    "    id_to_imgkeys[rid].sort()\n",
    "\n",
    "total_images = sum(len(v) for v in id_to_imgkeys.values())\n",
    "print(f\"[INFO] scanned image keys: {count_keys}\")\n",
    "print(f\"[INFO] unique recipeIds with images: {len(img_ids)}\")\n",
    "print(f\"[INFO] total image files matched to recipeIds: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ca2a4",
   "metadata": {},
   "source": [
    "## 4. Stream-filter docs.jsonl by img_ids and multipart-upload to Trusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b82f5",
   "metadata": {},
   "source": [
    "The recipes dataset (`recipes.jsonl`) can be extremely large.  \n",
    "Instead of checking every image individually, we use the set of `img_ids` extracted earlier to filter only the recipes that have at least one corresponding image.\n",
    "\n",
    "This step ensures that:\n",
    "- Every recipe in the Trusted Zone can be paired with one or more valid images.\n",
    "- Entries without visual data (recipes with missing images) are excluded, as they cannot contribute to multimodal analyses.\n",
    "\n",
    "The filtering is performed in a **streaming** manner using multipart uploads, which prevents memory overload even for very large JSONL files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf02264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote filtered docs to s3://trusted-zone/documents/recipes.jsonl\n",
      "[STATS] docs total=1029720 kept=7 dropped=1029713\n"
     ]
    }
   ],
   "source": [
    "MIN_PART_SIZE = 8 * 1024 * 1024  # 8 MB\n",
    "\n",
    "class MultipartJSONLWriter:\n",
    "    def __init__(self, bucket: str, key: str, content_type=\"application/x-ndjson\", metadata=None):\n",
    "        self.bucket = bucket\n",
    "        self.key = key\n",
    "        self.buf = io.BytesIO()\n",
    "        self.parts = []\n",
    "        self.part_num = 1\n",
    "        self.open = True\n",
    "        extra = {\n",
    "            \"Bucket\": bucket,\n",
    "            \"Key\": key,\n",
    "            \"ContentType\": content_type,\n",
    "            \"Metadata\": metadata or {},\n",
    "        }\n",
    "        resp = s3.create_multipart_upload(**extra)\n",
    "        self.upload_id = resp[\"UploadId\"]\n",
    "\n",
    "    def _flush_part(self):\n",
    "        self.buf.seek(0)\n",
    "        body = self.buf.read()\n",
    "        if not body:\n",
    "            self.buf.seek(0)\n",
    "            self.buf.truncate(0)\n",
    "            return\n",
    "        resp = s3.upload_part(\n",
    "            Bucket=self.bucket, Key=self.key,\n",
    "            UploadId=self.upload_id, PartNumber=self.part_num, Body=body\n",
    "        )\n",
    "        self.parts.append({\"ETag\": resp[\"ETag\"], \"PartNumber\": self.part_num})\n",
    "        self.part_num += 1\n",
    "        self.buf.seek(0); self.buf.truncate(0)\n",
    "\n",
    "    def write_line(self, raw_line_bytes: bytes):\n",
    "        # raw_line_bytes is already one JSON object line (no trailing \\n required)\n",
    "        self.buf.write(raw_line_bytes)\n",
    "        self.buf.write(b\"\\n\")\n",
    "        if self.buf.tell() >= MIN_PART_SIZE:\n",
    "            self._flush_part()\n",
    "\n",
    "    def close(self):\n",
    "        if not self.open:\n",
    "            return\n",
    "        try:\n",
    "            # If there’s leftover data, flush as a last part\n",
    "            self._flush_part()\n",
    "            if not self.parts:\n",
    "                # No data kept: abort multipart, optionally create empty object\n",
    "                s3.abort_multipart_upload(\n",
    "                    Bucket=self.bucket, Key=self.key, UploadId=self.upload_id\n",
    "                )\n",
    "                # Optional: write a 0-byte file so the path exists\n",
    "                s3.put_object(\n",
    "                    Bucket=self.bucket, Key=self.key, Body=b\"\",\n",
    "                    ContentType=\"application/x-ndjson\",\n",
    "                    Metadata={\"note\": \"empty after filtering\", \"ts\": utc_ts()},\n",
    "                )\n",
    "            else:\n",
    "                s3.complete_multipart_upload(\n",
    "                    Bucket=self.bucket, Key=self.key, UploadId=self.upload_id,\n",
    "                    MultipartUpload={\"Parts\": self.parts}\n",
    "                )\n",
    "        finally:\n",
    "            self.open = False\n",
    "\n",
    "def filter_docs_to_trusted_by_ids():\n",
    "    total = kept = 0\n",
    "    if DRY_RUN:\n",
    "        for raw in read_jsonl_lines(FORM_BUCKET, FORM_DOCS_KEY):\n",
    "            total += 1\n",
    "            try:\n",
    "                rid = json.loads(raw).get(\"id\")\n",
    "            except Exception:\n",
    "                continue\n",
    "            if rid in img_ids:\n",
    "                kept += 1\n",
    "        print(f\"[DRY_RUN] total={total} kept={kept}\")\n",
    "        return total, kept\n",
    "\n",
    "    writer = MultipartJSONLWriter(\n",
    "        TRUST_BUCKET, TRUST_DOCS_KEY,\n",
    "        content_type=\"application/x-ndjson\",\n",
    "        metadata={\"note\": \"filtered to ids that have images\", \"ts\": utc_ts()}\n",
    "    )\n",
    "    try:\n",
    "        for raw in read_jsonl_lines(FORM_BUCKET, FORM_DOCS_KEY):\n",
    "            total += 1\n",
    "            try:\n",
    "                rec = json.loads(raw)\n",
    "            except Exception:\n",
    "                continue\n",
    "            rid = rec.get(\"id\")\n",
    "            if rid and rid in img_ids:\n",
    "                kept += 1\n",
    "                writer.write_line(raw)\n",
    "    finally:\n",
    "        # Always close; it handles zero-kept gracefully\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"[OK] wrote filtered docs to s3://{TRUST_BUCKET}/{TRUST_DOCS_KEY}\")\n",
    "    return total, kept\n",
    "\n",
    "total_docs, kept_docs = filter_docs_to_trusted_by_ids()\n",
    "print(f\"[STATS] docs total={total_docs} kept={kept_docs} dropped={total_docs-kept_docs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96babe5",
   "metadata": {},
   "source": [
    "## 5. Copy only the kept images to Trusted, preserving filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086b0c9",
   "metadata": {},
   "source": [
    "Once the valid recipe entries have been identified, the corresponding images are copied from the **Formatted Zone** to the **Trusted Zone**.\n",
    "\n",
    "For each recipe ID, all associated images are transferred, preserving the original filenames.  \n",
    "This allows the dataset to maintain the full range of visual variants (different angles, styles, or preparation steps) linked to a single recipe.\n",
    "\n",
    "At this stage, only images that have a valid recipe entry are copied — any **orphan images** remain in the Formatted Zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e76556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATS] images copied=11 skipped=0\n"
     ]
    }
   ],
   "source": [
    "copied = skipped = 0\n",
    "\n",
    "if DRY_RUN:\n",
    "    print(\"[DRY_RUN] Skipping image copies.\")\n",
    "else:\n",
    "    # Only copy images for IDs that survived the filtering (i.e., in img_ids)\n",
    "    # Since we filtered docs by img_ids, all img_ids are \"kept\"\n",
    "    for rid, keys in id_to_imgkeys.items():\n",
    "        # keys are full 'images/...' relative keys in formatted-zone\n",
    "        for src_key in keys:\n",
    "            dst_key = f\"{TRUST_IMAGES_PREFIX}/{PurePosixPath(src_key).name}\"\n",
    "            try:\n",
    "                s3_copy_object(FORM_BUCKET, src_key, TRUST_BUCKET, dst_key, overwrite=OVERWRITE)\n",
    "                copied += 1\n",
    "            except ClientError as e:\n",
    "                print(f\"[WARN] copy failed {src_key} -> {dst_key}: {e}\")\n",
    "                skipped += 1\n",
    "\n",
    "print(f\"[STATS] images copied={copied} skipped={skipped}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517c274",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213648f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    \"ts\": utc_ts(),\n",
    "    \"source_docs\": f\"s3://{FORM_BUCKET}/{FORM_DOCS_KEY}\",\n",
    "    \"source_images_prefix\": f\"s3://{FORM_BUCKET}/{FORM_IMAGES_PREFIX}/\",\n",
    "    \"kept_doc_count\": kept_docs,\n",
    "    \"total_doc_count\": total_docs,\n",
    "    \"unique_recipe_ids_with_images\": len(img_ids),\n",
    "    \"images_copied\": copied,\n",
    "    \"images_skipped\": skipped\n",
    "}\n",
    "\n",
    "if not DRY_RUN:\n",
    "    s3.put_object(\n",
    "        Bucket=TRUST_BUCKET,\n",
    "        Key=f\"{TRUST_REPORT_PREFIX}/images_filter_{utc_ts()}.json\",\n",
    "        Body=json.dumps(report, ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "    print(f\"[OK] wrote report -> s3://{TRUST_BUCKET}/{TRUST_REPORT_PREFIX}/\")\n",
    "else:\n",
    "    print(\"[DRY_RUN] report:\", json.dumps(report, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
