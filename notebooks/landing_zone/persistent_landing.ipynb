{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb4be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import time\n",
    "import boto3\n",
    "import mimetypes\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import unquote\n",
    "from pathlib import PurePosixPath\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SRC_BUCKET      = \"landing-zone\"\n",
    "DEST_BUCKET     = SRC_BUCKET\n",
    "IMG_PREFIX      = \"persistent_landing/images\"\n",
    "DOC_PREFIX      = \"persistent_landing/documents\"\n",
    "HF_DATASET=os.getenv(\"HF_DATASET\")\n",
    "\n",
    "MINIO_USER=os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD=os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT=os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "DELETE_SOURCE_AFTER_COPY = True \n",
    "\n",
    "IMAGE_MIME_PREFIXES = (\"image/\",)\n",
    "IMAGE_EXTS = {\"jpg\", \"jpeg\", \"png\", \"gif\", \"webp\", \"bmp\", \"tiff\"}\n",
    "DOC_EXTS   = {\"json\", \"jsonl\", \"ndjson\"}\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\",\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c6790",
   "metadata": {},
   "source": [
    "This block loads environment variables and sets up a MinIO connection using **boto3**. It defines source and destination buckets, prefixes for images and documents, and basic file type filters for images and JSON files.\n",
    "\n",
    "The script also includes a flag to optionally delete source files after copying, supporting cleanup operations. Overall, this section prepares the environment and storage connection for later steps that organize and move files within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934f938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_ts() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n",
    "\n",
    "\n",
    "def guess_name_and_ext(key: str, head: dict) -> tuple[str, str]:\n",
    "    p = PurePosixPath(key)\n",
    "    name = p.name\n",
    "    base = p.stem or \"file\"\n",
    "    ext = p.suffix.lower().lstrip(\".\")\n",
    "\n",
    "    if not ext:\n",
    "        ctype = (head.get(\"ContentType\") or \"\").split(\";\")[0].strip().lower()\n",
    "        if ctype:\n",
    "            guess = mimetypes.guess_extension(ctype) or \"\"\n",
    "            ext = guess.lstrip(\".\")\n",
    "            if ext == \"jpe\":\n",
    "                ext = \"jpg\"\n",
    "    if ext == \"jpeg\":\n",
    "        ext = \"jpg\"\n",
    "    return base, ext or \"bin\"\n",
    "\n",
    "\n",
    "def is_image(head: dict, ext: str) -> bool:\n",
    "    ctype = (head.get(\"ContentType\") or \"\").lower()\n",
    "    return ctype.startswith(IMAGE_MIME_PREFIXES) or ext in IMAGE_EXTS\n",
    "\n",
    "\n",
    "def is_document_json(head: dict, ext: str) -> bool:\n",
    "    ctype = (head.get(\"ContentType\") or \"\").split(\";\")[0].strip().lower()\n",
    "    return ext in DOC_EXTS or ctype == \"application/json\"\n",
    "\n",
    "\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    return re.sub(r\"[^\\w\\-.]+\", \"_\", s)\n",
    "\n",
    "\n",
    "def make_target_key(obj_type: str, dataset: str, ts: str, filename: str, ext: str, prefix: str) -> str:\n",
    "    filename = sanitize_filename(filename)\n",
    "    dataset  = sanitize_filename(dataset)\n",
    "    return f\"{prefix}/{obj_type}${dataset}${ts}${filename}.{ext}\"\n",
    "\n",
    "\n",
    "def copy_object(src_bucket: str, src_key: str, dst_bucket: str, dst_key: str, metadata: dict | None = None, content_type: str | None = None):\n",
    "    extra = {\"MetadataDirective\": \"REPLACE\"}\n",
    "    if metadata:\n",
    "        extra[\"Metadata\"] = metadata\n",
    "    if content_type:\n",
    "        extra[\"ContentType\"] = content_type\n",
    "\n",
    "    s3.copy_object(\n",
    "        CopySource={\"Bucket\": src_bucket, \"Key\": src_key},\n",
    "        Bucket=dst_bucket,\n",
    "        Key=dst_key,\n",
    "        **extra,\n",
    "    )\n",
    "\n",
    "\n",
    "def move_or_copy(src_bucket: str, src_key: str, dst_bucket: str, dst_key: str, **kwargs):\n",
    "    copy_object(src_bucket, src_key, dst_bucket, dst_key, **kwargs)\n",
    "    if DELETE_SOURCE_AFTER_COPY:\n",
    "        try:\n",
    "            s3.delete_object(Bucket=src_bucket, Key=src_key)\n",
    "        except ClientError as e:\n",
    "            print(f\"[WARN] failed to delete from origin {src_key}: {e}\")\n",
    "            \n",
    "            \n",
    "def atomic_write_json(path: str, data: dict):\n",
    "    tmp = f\"{path}.tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(data, fh, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa5254",
   "metadata": {},
   "source": [
    "This block defines utility functions for classifying, naming, and moving files between MinIO buckets.\n",
    "\n",
    "`utc_ts` generates a UTC timestamp used to version or label copied objects. `guess_name_and_ext` extracts or infers a file’s base name and extension from its key or content type, normalizing image formats like `.jpeg` to `.jpg`. The functions `is_image` and `is_document_json` classify files by checking MIME types and extensions.\n",
    "\n",
    "`sanitize_filename` cleans names to avoid invalid characters, while `make_target_key` builds standardized keys using the pattern `type$dataset$timestamp$name.extension`, ensuring unique and descriptive filenames.\n",
    "\n",
    "Finally, `copy_object` and `move_or_copy` handle the actual file transfer, replacing metadata if needed and optionally deleting the source file after copying, supporting a clean migration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e05039bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] persistent_landing/documents/document$adsdb-multimodal-food-data-management$2025-10-11T17-15-55Z$803760260afbeee8__det_ingrs.json (ctype=application/json, ext=.json)\n",
      "[SKIP] persistent_landing/documents/document$adsdb-multimodal-food-data-management$2025-10-11T17-15-55Z$a0ad8838a448b589__recipes_with_nutritional_info.json (ctype=application/json, ext=.json)\n",
      "[SKIP] persistent_landing/documents/document$adsdb-multimodal-food-data-management$2025-10-11T17-15-55Z$bdcc01219cf0d860__layer1.json (ctype=application/json, ext=.json)\n",
      "[IMG] temporal_landing/19214085d36fb535c7dbf24d178dea1b__000095fc1d_0.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$19214085d36fb535c7dbf24d178dea1b__000095fc1d_0.jpg\n",
      "[IMG] temporal_landing/1a426726a1e467b8e6360e980b03f611__00003a70b1_1.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$1a426726a1e467b8e6360e980b03f611__00003a70b1_1.jpg\n",
      "[IMG] temporal_landing/1b1a47ccbb9d3b879c21952c03111b34__00003a70b1_2.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$1b1a47ccbb9d3b879c21952c03111b34__00003a70b1_2.jpg\n",
      "[IMG] temporal_landing/4786052a58187b0a9c875fde7cf940c9__00010c7867_0.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$4786052a58187b0a9c875fde7cf940c9__00010c7867_0.jpg\n",
      "[IMG] temporal_landing/6aaf18e8462526c1298ea0ccfaa861a5__0000b1e2b5_0.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$6aaf18e8462526c1298ea0ccfaa861a5__0000b1e2b5_0.jpg\n",
      "[IMG] temporal_landing/6e0720efb93c1e5c1719b5b4c0c4cd7e__0000c79afb_0.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$6e0720efb93c1e5c1719b5b4c0c4cd7e__0000c79afb_0.jpg\n",
      "[IMG] temporal_landing/89e502dee0f72c8b4b9382d5566e84c3__00010d44c7_0.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$89e502dee0f72c8b4b9382d5566e84c3__00010d44c7_0.jpg\n",
      "[IMG] temporal_landing/9e7c94ba98c15fd4bf9c2014acc3a8d6__00007bfd16_0.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$9e7c94ba98c15fd4bf9c2014acc3a8d6__00007bfd16_0.jpg\n",
      "[IMG] temporal_landing/a558f045aadaf96e44b67c18e77b0a87__00010c7867_1.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$a558f045aadaf96e44b67c18e77b0a87__00010c7867_1.jpg\n",
      "[IMG] temporal_landing/b07c43b0325f168d7ce2b580d8e4b2a4__00003a70b1_0.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$b07c43b0325f168d7ce2b580d8e4b2a4__00003a70b1_0.jpg\n",
      "[IMG] temporal_landing/f048b362a2cd8db702bf00f52307e6d5__00007bfd16_1.jpg -> s3://landing-zone/persistent_landing/images/image$adsdb-multimodal-food-data-management$2025-10-11T17-54-44Z$f048b362a2cd8db702bf00f52307e6d5__00007bfd16_1.jpg\n",
      "\n",
      "[INDEX] imagenes indexadas: 11 -> image_index.json\n",
      "\n",
      "[STATS] total=14  images=11  documents=0  skipped=3\n"
     ]
    }
   ],
   "source": [
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "ing_ts = utc_ts()\n",
    "\n",
    "pages = paginator.paginate(Bucket=SRC_BUCKET)\n",
    "total = moved_img = moved_doc = skipped = 0\n",
    "\n",
    "images_index = {}\n",
    "IMAGE_INDEX_PATH = \"image_index.json\"\n",
    "\n",
    "for page in pages:\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        total += 1\n",
    "\n",
    "        if key.endswith(\"/\") or key.startswith(\".\"):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            head = s3.head_object(Bucket=SRC_BUCKET, Key=key)\n",
    "        except ClientError as e:\n",
    "            print(f\"[WARN] head_object failed in {key}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        base, ext = guess_name_and_ext(key, head)\n",
    "\n",
    "        if is_image(head, ext):\n",
    "            dst_key = make_target_key(\"image\", HF_DATASET, ing_ts, base, ext, prefix=IMG_PREFIX)\n",
    "            move_or_copy(\n",
    "                SRC_BUCKET, key, DEST_BUCKET, dst_key,\n",
    "                metadata={\n",
    "                    \"src-bucket\": SRC_BUCKET,\n",
    "                    \"src-key\": key,\n",
    "                    \"dataset\": HF_DATASET,\n",
    "                    \"ingestion-ts\": ing_ts,\n",
    "                },\n",
    "                content_type=head.get(\"ContentType\"),\n",
    "            )\n",
    "            moved_img += 1\n",
    "            print(f\"[IMG] {key} -> s3://{DEST_BUCKET}/{dst_key}\")\n",
    "            \n",
    "            dst_name = PurePosixPath(dst_key).name\n",
    "            id_part = re.search(r'_([A-Fa-f0-9]+_[0-9]+)\\.[^.]+$', dst_name)\n",
    "            image_id = id_part.group(1) if id_part else dst_name\n",
    "\n",
    "            images_index[key] = {\n",
    "                \"id\": image_id,\n",
    "            }\n",
    "\n",
    "        elif is_document_json(head, ext) and False:\n",
    "            dst_key = make_target_key(\"document\", HF_DATASET, ing_ts, base, ext, prefix=DOC_PREFIX)\n",
    "            move_or_copy(\n",
    "                SRC_BUCKET, key, DEST_BUCKET, dst_key,\n",
    "                metadata={\n",
    "                    \"src-bucket\": SRC_BUCKET,\n",
    "                    \"src-key\": key,\n",
    "                    \"dataset\": HF_DATASET,\n",
    "                    \"ingestion-ts\": ing_ts,\n",
    "                },\n",
    "                content_type=head.get(\"ContentType\") or \"application/json\",\n",
    "            )\n",
    "            moved_doc += 1\n",
    "            print(f\"[DOC] {key} -> s3://{DEST_BUCKET}/{dst_key}\")\n",
    "\n",
    "        else:\n",
    "            skipped += 1\n",
    "            print(f\"[SKIP] {key} (ctype={head.get('ContentType')}, ext=.{ext})\")\n",
    "\n",
    "if images_index:\n",
    "    try:\n",
    "        atomic_write_json(IMAGE_INDEX_PATH, images_index)\n",
    "        print(f\"\\n[INDEX] imagenes indexadas: {len(images_index)} -> {IMAGE_INDEX_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] no se pudo escribir el índice local {IMAGE_INDEX_PATH}: {e}\")\n",
    "\n",
    "print(f\"\\n[STATS] total={total}  images={moved_img}  documents={moved_doc}  skipped={skipped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8027f6",
   "metadata": {},
   "source": [
    "This block scans all objects in the source MinIO bucket and organizes them into structured destinations based on file type.\n",
    "\n",
    "It uses a paginator to iterate through every stored object, skipping directories or hidden files. For each valid file, the script retrieves metadata, determines its type, and applies the standardized naming convention `type$dataset$timestamp$name.extension`.\n",
    "\n",
    "Image files are moved under the image prefix, and JSON documents under the document prefix — both enriched with metadata such as source path, dataset name, and ingestion timestamp. Unsupported or unrecognized files are skipped.\n",
    "\n",
    "Finally, summary statistics are printed, showing totals for processed, moved, and skipped files, providing a quick overview of the ingestion process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
