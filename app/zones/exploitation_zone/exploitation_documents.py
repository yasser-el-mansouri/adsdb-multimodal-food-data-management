"""
Exploitation Zone - Documents Processing

This module handles the text processing step for the Exploitation Zone.
It generates embeddings from documents and stores them in ChromaDB.
"""

import gzip
import json
import os
from typing import Any, Dict, Iterable, List, Optional, Tuple

import chromadb
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction, OpenCLIPEmbeddingFunction

# Import shared utilities
from app.utils.shared import Logger, PipelineConfig, S3Client, error_handler, utc_timestamp


class ExploitationDocumentsProcessor:
    """Processor for exploitation zone documents."""

    def __init__(self, config: PipelineConfig):
        """Initialize the processor."""
        self.config = config
        self.logger = Logger("exploitation_documents", config.get("monitoring.log_level", "INFO"))
        self.s3_client = S3Client(config)
        # Configuration
        self.src_bucket = config.get("storage.buckets.trusted_zone")
        self.src_prefix = config.get("storage.prefixes.trusted_documents")

        # ChromaDB configuration
        chromadb_config = config.get("chromadb_documents", {})
        self.collection_name = chromadb_config.get("collection_name", "exploitation_documents")
        self.embedding_model = chromadb_config.get("embedding_model", "Qwen/Qwen3-Embedding-0.6B")
        self.metadata = chromadb_config.get("metadata", {})
        self.persist_dir = chromadb_config.get(
            "persist_dir", "app/zones/exploitation_zone/chroma_documents"
        )

        chromadb_config_multi = config.get("chromadb_multimodal", {})
        self.collection_name_multi = chromadb_config_multi.get("collection_name", "exploitation_multimodal")
        self.embedding_model_multi = chromadb_config_multi.get("embedding_model", "OpenCLIP")
        self.metadata_multi = chromadb_config_multi.get("metadata", {})
        self.persist_dir_multi = chromadb_config_multi.get(
            "persist_dir", "app/zones/exploitation_zone/chroma_multimodal"
        )

        # Processing configuration
        self.batch_size = config.get("pipeline.batch_size", 256)

        # Initialize ChromaDB client and embedding function
        self.chroma_client = PersistentClient(path=self.persist_dir)
        self.ef_text = SentenceTransformerEmbeddingFunction(model_name=self.embedding_model)
        self.ef_multi = OpenCLIPEmbeddingFunction()

        # Initialize ChromaDB collection
        collection_kwargs = {"name": self.collection_name, "embedding_function": self.ef_text}
        multi_collection_kwargs = {"name": self.collection_name_multi, "embedding_function": self.ef_multi}

        # Only add metadata if it's not empty
        if self.metadata:
            collection_kwargs["metadata"] = self.metadata
        if self.metadata_multi:
            multi_collection_kwargs["metadata"] = self.metadata_multi

        # Delete existing collections to ensure clean state
        try:
            self.chroma_client.delete_collection(name=self.collection_name)
            self.logger.info(f"Deleted existing collection: {self.collection_name}")
        except Exception:
            pass  # Collection doesn't exist, that's fine
        
        try:
            self.chroma_client.delete_collection(name=self.collection_name_multi)
            self.logger.info(f"Deleted existing collection: {self.collection_name_multi}")
        except Exception:
            pass  # Collection doesn't exist, that's fine

        # Create fresh collections
        self.recipes_col = self.chroma_client.create_collection(**collection_kwargs)
        self.multi_col = self.chroma_client.create_collection(**multi_collection_kwargs)

        self.logger.info(f"ChromaDB directory: {self.persist_dir}")

    def list_all_recipe_keys(self, bucket: str, prefix: str) -> List[str]:
        """List all recipe file keys in bucket with prefix."""
        keys = []
        exts = (".jsonl",)

        paginator = self.s3_client.client.get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get("Contents", []):
                k = obj["Key"]
                if k.lower().endswith(exts):
                    keys.append(k)
                    self.logger.info(f"Found recipe file: {k}")

        return keys

    def stream_objects_lines(self, bucket: str, key: str) -> Iterable[str]:
        """Stream lines from S3 object."""
        obj = self.s3_client.get_object(bucket=bucket, key=key)
        raw = obj["Body"].read()

        if key.lower().endswith(".gz"):
            raw = gzip.decompress(raw)

        text = raw.decode("utf-8", errors="replace")
        if key.lower().endswith(".jsonl"):
            for line in text.splitlines():
                line = line.strip()
                if line:
                    yield line

    def normalize_recipe_json(self, rec: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Normalize recipe JSON for embedding."""
        rid = str(rec.get("id") or "").strip()
        if not rid:
            return None

        # Extract title
        title = (
            rec.get("title_text_clean")
            or rec.get("title__from_recipes_with_nutritional_info")
            or rec.get("title__from_layer1")
            or rec.get("title_text_raw")
            or ""
        ).strip()

        # Extract ingredients
        ing_clean = rec.get("ingredients_text_clean")
        if not ing_clean:
            ing_lists = (
                rec.get("ingredients__from_recipes_with_nutritional_info")
                or rec.get("ingredients__from_layer1")
                or rec.get("ingredients__from_det_ingrs")
                or []
            )
            ing_clean = " ".join(
                x.get("text", "").strip() for x in ing_lists if isinstance(x, dict)
            )

        # Extract instructions
        instr_clean = rec.get("instructions_text_clean")
        if not instr_clean:
            instr_lists = (
                rec.get("instructions__from_recipes_with_nutritional_info")
                or rec.get("instructions__from_layer1")
                or []
            )
            instr_clean = " ".join(
                x.get("text", "").strip() for x in instr_lists if isinstance(x, dict)
            )

        # Build document text
        parts = []
        if title:
            parts.append(f"title: {title}")
        if ing_clean:
            parts.append(f"ingredients: {ing_clean}")
        if instr_clean:
            parts.append(f"instructions: {instr_clean}")

        document = "\n".join(parts).strip()
        if not document:
            return None

        # Extract FSA lights
        lights = rec.get("fsa_lights_per100g__from_recipes_with_nutritional_info") or {}

        # Build metadata
        meta = {
            "id": rid,
            "title": title,
            "has_nutrition": bool(rec.get("nutrition_normalized")),
            "source_bucket": "trusted-zone",
        }

        # Add FSA values if they're not None
        if lights.get("fat") is not None:
            meta["fsa_fat"] = lights.get("fat")
        if lights.get("salt") is not None:
            meta["fsa_salt"] = lights.get("salt")
        if lights.get("saturates") is not None:
            meta["fsa_saturates"] = lights.get("saturates")
        if lights.get("sugars") is not None:
            meta["fsa_sugars"] = lights.get("sugars")

        return {"id": rid, "document": document, "metadata": meta}

    def ingest_recipes(self, bucket: str, prefix: str, batch: int = None, collection: Any = None) -> Dict[str, Any]:
        """Ingest recipes from S3 into ChromaDB."""
        if batch is None:
            batch = self.batch_size

        buf_ids, buf_docs, buf_meta = [], [], []
        n_ok, n_bad = 0, 0

        keys = self.list_all_recipe_keys(bucket, prefix)

        for key in keys:
            self.logger.info(f"Processing file: {key}")

            for payload in self.stream_objects_lines(bucket, key):
                try:
                    data = json.loads(payload)
                    if isinstance(data, dict):
                        items = [data]
                    elif isinstance(data, list):
                        items = data
                    else:
                        raise ValueError("JSON root is not a dict or list")

                    for rec in items:
                        if not isinstance(rec, dict):
                            n_bad += 1
                            continue

                        pack = self.normalize_recipe_json(rec)
                        if not pack:
                            n_bad += 1
                            continue

                        buf_ids.append(pack["id"])
                        buf_docs.append(pack["document"])
                        buf_meta.append({"type": "text"} | pack["metadata"])
                        n_ok += 1

                        if len(buf_ids) >= batch:
                            collection.add(
                                ids=buf_ids, documents=buf_docs, metadatas=buf_meta
                            )
                            self.logger.info(f"Added batch of {len(buf_ids)} recipes to ChromaDB")
                            buf_ids, buf_docs, buf_meta = [], [], []

                except Exception as e:
                    self.logger.warning(f"Error processing payload: {e}")
                    n_bad += 1

        # Add remaining items
        if buf_ids:
            collection.add(ids=buf_ids, documents=buf_docs, metadatas=buf_meta)
            self.logger.info(f"Added final batch of {len(buf_ids)} recipes to ChromaDB")

        self.logger.info(f"Ingestion completed. OK: {n_ok} | discarded: {n_bad}")

        return {"processed_records": n_ok, "discarded_records": n_bad, "total_files": len(keys)}

    def test_query(
        self, query: str = "honey sriracha chicken wings", n_results: int = 1
    ) -> Dict[str, Any]:
        """Test query against the collection."""
        try:
            res = self.recipes_col.query(
                query_texts=[query],
                n_results=n_results,
                include=["metadatas", "distances", "documents"],
            )

            self.logger.info(f"Query: {query}")
            self.logger.info(f"Results: {json.dumps(res, indent=2, ensure_ascii=False)}")

            return res
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            return {}

    def process(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """Main processing method."""
        try:
            with error_handler(self.logger, "exploitation_documents_processing"):
                # Ingest recipes
                ingestion_stats = self.ingest_recipes(self.src_bucket, self.src_prefix, collection=self.recipes_col)
                ingestion_stats_multi = self.ingest_recipes(self.src_bucket, self.src_prefix, collection=self.multi_col)

                # Test query
                query_results = self.test_query()

                result = {
                    **ingestion_stats,
                    "timestamp": utc_timestamp(),
                    "collection_name": self.collection_name,
                    "embedding_model": self.embedding_model,
                }
                multi_result = {
                    **ingestion_stats_multi,
                    "timestamp": utc_timestamp(),
                    "collection_name": self.collection_name_multi,
                    "embedding_model": self.embedding_model_multi,
                }

                self.logger.info(f"Exploitation documents processing completed: {result}")
                self.logger.info(f"Exploitation multimodal document processing completed: {multi_result}")
                return result

        except Exception as e:
            self.logger.error(f"Exploitation documents processing failed: {e}")
            raise


def main():
    """Main entry point for exploitation documents processing."""
    config = PipelineConfig()
    processor = ExploitationDocumentsProcessor(config)

    try:
        result, result_multi = processor.process()
        print(f"✅ Exploitation documents processing completed successfully")
        print(f"📊 Metrics: {result}")
        print(f"📊 Multimodal Metrics: {result_multi}")
    except Exception as e:
        print(f"❌ Exploitation documents processing failed: {e}")
        raise


if __name__ == "__main__":
    main()
