{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2232a9",
   "metadata": {},
   "source": [
    "# 1) Setup env, MinIO client, Chroma client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a277a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from c:\\Users\\sindr\\Documents\\FIB\\adsdb-multimodal-food-data-management\\.env\n",
      "Available collections: ['trusted_zone_multimodal', 'trusted_zone_images', 'trusted_zone_documents']\n",
      "✅ Multi-modal collection ready: trusted_zone_multimodal\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, hashlib\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import Iterable, Dict, Any, List, Optional\n",
    "import numpy\n",
    "\n",
    "dotenv_path = find_dotenv(filename='.env', usecwd=True)\n",
    "if not dotenv_path:\n",
    "    raise FileNotFoundError(\"Could not find .env. Set its path manually.\")\n",
    "print(f\"Loading environment variables from {dotenv_path}\")\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import chromadb\n",
    "\n",
    "# --- ENV ---\n",
    "TRUSTED_BUCKET       = os.environ.get(\"TRUSTED_BUCKET\", \"trusted-zone\")\n",
    "\n",
    "CHROMA_PERSIST_DIR   = os.environ.get(\"CHROMA_PERSIST_DIR\", \"exploitation_zone/chroma\")\n",
    "\n",
    "# --- MinIO S3 client ---\n",
    "MINIO_USER     = os.environ.get(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.environ.get(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.environ.get(\"MINIO_ENDPOINT\")\n",
    "\n",
    "# Paths and Buckets\n",
    "TRUST_BUCKET        = \"trusted-zone\"\n",
    "TRUST_IMAGES_PREFIX = \"images\"\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
    ")\n",
    "\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "\n",
    "# Connect to the same local exploitation directory\n",
    "CHROMA = chromadb.PersistentClient(path=\"../\" + CHROMA_PERSIST_DIR)\n",
    "\n",
    "# Use CLIP for both text & images\n",
    "ef_clip = OpenCLIPEmbeddingFunction()\n",
    "\n",
    "# list all existing collections\n",
    "existing_names = [col.name for col in CHROMA.list_collections()]\n",
    "print(\"Available collections:\", existing_names)\n",
    "\n",
    "target_name = \"trusted_zone_multimodal\"\n",
    "\n",
    "# connect to the collection for multi-modal data\n",
    "multi_col = CHROMA.get_or_create_collection(\n",
    "    name=\"trusted_zone_multimodal\",\n",
    "    embedding_function=ef_clip,\n",
    "    metadata={\"modality\": \"image+text\", \"model\": \"OpenCLIP\", \"source\": \"minio\"}\n",
    ")\n",
    "\n",
    "print(\"✅ Multi-modal collection ready:\", multi_col.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10018a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available buckets: ['formatted-zone', 'landing-zone', 'trusted-zone']\n",
      "✅ Bucket 'trusted-zone' exists\n",
      "Objects in trusted-zone: 53\n"
     ]
    }
   ],
   "source": [
    "# Test MinIO connection and bucket\n",
    "try:\n",
    "    # List all buckets\n",
    "    response = s3.list_buckets()\n",
    "    print(\"Available buckets:\", [b['Name'] for b in response['Buckets']])\n",
    "    \n",
    "    # Check if trusted-zone bucket exists\n",
    "    if TRUSTED_BUCKET in [b['Name'] for b in response['Buckets']]:\n",
    "        print(f\"✅ Bucket '{TRUSTED_BUCKET}' exists\")\n",
    "    else:\n",
    "        print(f\"❌ Bucket '{TRUSTED_BUCKET}' does NOT exist\")\n",
    "        \n",
    "    # Try to list objects in the bucket\n",
    "    try:\n",
    "        objects = s3.list_objects_v2(Bucket=TRUSTED_BUCKET)\n",
    "        print(f\"Objects in {TRUSTED_BUCKET}: {objects.get('KeyCount', 0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cannot access bucket {TRUSTED_BUCKET}: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"MinIO error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080dcff",
   "metadata": {},
   "source": [
    "# 2) Retrieval helpers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d546e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "\n",
    "_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Match Chroma's default OpenCLIPEmbeddingFunction() config\n",
    "_MODEL_NAME = \"ViT-B-32\"\n",
    "_PRETRAINED = \"laion2b_s34b_b79k\"\n",
    "\n",
    "_CLIP_MODEL, _CLIP_PREPROCESS, _ = open_clip.create_model_and_transforms(\n",
    "    _MODEL_NAME,\n",
    "    pretrained=_PRETRAINED,\n",
    "    device=_DEVICE\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_image_to_vec(pil_img: Image.Image) -> list[float]:\n",
    "    \"\"\"\n",
    "    Produce an OpenCLIP image embedding compatible with the vectors stored in\n",
    "    the 'trusted_zone_images' Chroma collection.\n",
    "    \"\"\"\n",
    "    img_tensor = _CLIP_PREPROCESS(pil_img).unsqueeze(0).to(_DEVICE)\n",
    "    img_features = _CLIP_MODEL.encode_image(img_tensor)\n",
    "    img_features = img_features / img_features.norm(dim=-1, keepdim=True)\n",
    "    return img_features.squeeze(0).cpu().tolist()\n",
    "\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_retrieved_images(local_query_path, hits):\n",
    "    \"\"\"Show the query image followed by the retrieved similar images.\"\"\"\n",
    "    # --- display the query image ---\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.subplot(1, len(hits) + 1, 1)\n",
    "    query_img = Image.open(local_query_path).convert(\"RGB\")\n",
    "    plt.imshow(query_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Query\")\n",
    "\n",
    "    # --- display each retrieved image from MinIO ---\n",
    "    for i, hit in enumerate(hits, start=2):\n",
    "        bucket = hit[\"image_s3_bucket\"]\n",
    "        key = hit[\"image_s3_key\"]\n",
    "        score = hit[\"score\"]\n",
    "\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        img = Image.open(io.BytesIO(obj[\"Body\"].read())).convert(\"RGB\")\n",
    "\n",
    "        plt.subplot(1, len(hits) + 1, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{i-1}. Distance={score:.3f}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def show_text_results(result):\n",
    "    print(f\"🔎 Query: {result['query']}\\n\")\n",
    "    for i, hit in enumerate(result[\"hits\"], start=1):\n",
    "        text_preview = textwrap.shorten(hit[\"text\"], width=180, placeholder=\"…\")\n",
    "        print(f\"{i}.  Distance={hit['score']:.3f}\")\n",
    "        print(text_preview)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# --- Compute summary statistics ---\n",
    "def summarize(label, arr):\n",
    "    if not arr:\n",
    "        print(f\"No {label} results found.\")\n",
    "        return None, None\n",
    "    return min(arr), max(arr)\n",
    "\n",
    "def print_multi_summary(res: Dict[str, Any]):\n",
    "    metas = res[\"metadatas\"][0]\n",
    "    dists = res[\"distances\"][0]\n",
    "\n",
    "    image_dists = [d for m, d in zip(metas, dists) if m.get(\"type\") == \"image\"]\n",
    "    text_dists  = [d for m, d in zip(metas, dists) if m.get(\"type\") == \"text\"]\n",
    "\n",
    "    closest_img, farthest_img = summarize(\"image\", image_dists)\n",
    "    closest_txt, farthest_txt = summarize(\"text\", text_dists)\n",
    "\n",
    "    # --- Print the summary neatly ---\n",
    "    print(\"🔍 Cross-Modal Query Summary\")\n",
    "    print(f\"Closest image match has distance  {closest_img:.3f}\")\n",
    "    print(f\"Farthest image match has distance {farthest_img:.3f}\")\n",
    "    print(f\"Closest recipe match has distance {closest_txt:.3f}\")\n",
    "    print(f\"Farthest recipe match has distance {farthest_txt:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb363aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction, SentenceTransformerEmbeddingFunction\n",
    "\n",
    "ef_clip = OpenCLIPEmbeddingFunction()\n",
    "\n",
    "def get_multi_collection():\n",
    "    return client.get_collection(\n",
    "        name=\"trusted_zone_multimodal\",\n",
    "        embedding_function=ef_clip,\n",
    "    )\n",
    "\n",
    "def retrieve_query(query: str, k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k similar text docs from Chroma for a given natural language query.\n",
    "    Returns a dict with 'hits' = [{text, meta, score}, ...]\n",
    "    \"\"\"\n",
    "    col = get_multi_collection()\n",
    "\n",
    "    result = col.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "    \n",
    "    hits = []\n",
    "    docs = result.get(\"documents\", [[]])[0]\n",
    "    metas = result.get(\"metadatas\", [[]])[0]\n",
    "    dists = result.get(\"distances\", [[]])[0]\n",
    "\n",
    "    for text_doc, meta, dist in zip(docs, metas, dists):\n",
    "        hits.append({\n",
    "            \"text\": text_doc,\n",
    "            \"meta\": meta,\n",
    "            \"score\": float(dist),\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"hits\": hits,\n",
    "    }\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_top_images_as_base64(image_hits, max_images=3):\n",
    "    \"\"\"\n",
    "    Takes the image_hits list from retrieve_images() (with bucket/key),\n",
    "    downloads up to max_images from MinIO,\n",
    "    converts to JPEG,\n",
    "    returns a list of base64-encoded strings ready for Ollama llava.\n",
    "    \"\"\"\n",
    "    b64_list = []\n",
    "\n",
    "    for i, hit in enumerate(image_hits):\n",
    "        if i >= max_images:\n",
    "            break\n",
    "\n",
    "        bucket = hit[\"image_s3_bucket\"]\n",
    "        key = hit[\"image_s3_key\"]\n",
    "\n",
    "        # download from MinIO\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        img_bytes = obj[\"Body\"].read()\n",
    "\n",
    "        # normalize to JPEG bytes in memory\n",
    "        pil_img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "        buf = io.BytesIO()\n",
    "        pil_img.save(buf, format=\"JPEG\", quality=90)\n",
    "        jpeg_bytes = buf.getvalue()\n",
    "\n",
    "        # base64 encode for Ollama\n",
    "        b64_img = base64.b64encode(jpeg_bytes).decode(\"utf-8\")\n",
    "        b64_list.append({\n",
    "            \"bucket\": bucket,\n",
    "            \"key\": key,\n",
    "            \"b64\": b64_img,\n",
    "        })\n",
    "\n",
    "    return b64_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b00cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "  - trusted_zone_multimodal\n",
      "  - trusted_zone_images\n",
      "  - trusted_zone_documents\n",
      "\n",
      "Collection 'trusted_zone_multimodal' has 85 entries\n",
      "Sample entry:\n",
      "Error accessing collection: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what collections exist\n",
    "print(\"Available collections:\")\n",
    "for col_name in CHROMA.list_collections():\n",
    "    print(f\"  - {col_name.name}\")\n",
    "    \n",
    "# Check the specific collection\n",
    "try:\n",
    "    col = CHROMA.get_collection(\"trusted_zone_multimodal\")\n",
    "    count = col.count()\n",
    "    print(f\"\\nCollection 'trusted_zone_multimodal' has {count} entries\")\n",
    "    \n",
    "    if count > 0:\n",
    "        # Get a sample\n",
    "        sample = col.get(limit=2)\n",
    "        print(\"Sample entry:\")\n",
    "        for i, doc in enumerate(sample['documents'][:2]):\n",
    "            print(f\"[{i+1}] {doc[:200]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing collection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32404d",
   "metadata": {},
   "source": [
    "# 3) Text search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90891b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 85 | Images: 0 | Texts: 0\n",
      "<chromadb.utils.embedding_functions.open_clip_embedding_function.OpenCLIPEmbeddingFunction object at 0x000001C40E667D70>\n",
      "No image results found.\n",
      "No text results found.\n",
      "🔍 Cross-Modal Query Summary\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     11\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mfettuccine alfredo pasta dish with creamy sauce\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m res = multi_col.query(\n\u001b[32m     14\u001b[39m     query_texts=[query],\n\u001b[32m     15\u001b[39m     n_results=\u001b[32m85\u001b[39m,\n\u001b[32m     16\u001b[39m     include=[\u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mprint_multi_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mprint_multi_summary\u001b[39m\u001b[34m(res)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# --- Print the summary neatly ---\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔍 Cross-Modal Query Summary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClosest image match has distance  \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclosest_img\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.3f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFarthest image match has distance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfarthest_img\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClosest recipe match has distance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclosest_txt\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "all_data = multi_col.get(limit=10000)\n",
    "metas = all_data[\"metadatas\"]\n",
    "\n",
    "image_count = sum(1 for m in metas if m.get(\"type\") == \"image\")\n",
    "text_count  = sum(1 for m in metas if m.get(\"type\") == \"text\")\n",
    "\n",
    "print(f\"Total items: {len(metas)} | Images: {image_count} | Texts: {text_count}\")\n",
    "\n",
    "print(multi_col._embedding_function)\n",
    "\n",
    "query = \"fettuccine alfredo pasta dish with creamy sauce\"\n",
    "\n",
    "res = multi_col.query(\n",
    "    query_texts=[query],\n",
    "    n_results=85,\n",
    "    include=[\"metadatas\", \"documents\", \"distances\"]\n",
    ")\n",
    "\n",
    "print_multi_summary(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d6792",
   "metadata": {},
   "source": [
    "# 4) Image search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b633882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 135 | Images: 50 | Texts: 35\n",
      "<chromadb.utils.embedding_functions.open_clip_embedding_function.OpenCLIPEmbeddingFunction object at 0x00000153E21092B0>\n",
      "No text results found.\n",
      "🔍 Cross-Modal Query Summary\n",
      "Closest image match has distance  0.361\n",
      "Farthest image match has distance 0.654\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     12\u001b[39m query = np.array(Image.open(\u001b[33m\"\u001b[39m\u001b[33mcalico-beans.jpg\u001b[39m\u001b[33m\"\u001b[39m).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     14\u001b[39m res = multi_col.query(\n\u001b[32m     15\u001b[39m     query_images=[query],\n\u001b[32m     16\u001b[39m     n_results=\u001b[32m85\u001b[39m,    \u001b[38;5;66;03m# top-k results\u001b[39;00m\n\u001b[32m     17\u001b[39m     include=[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdistances\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mprint_multi_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mprint_multi_summary\u001b[39m\u001b[34m(res)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClosest image match has distance  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclosest_img\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFarthest image match has distance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfarthest_img\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClosest recipe match has distance \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclosest_txt\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.3f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFarthest recipe match has distance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfarthest_txt\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_data = multi_col.get(limit=10000)\n",
    "metas = all_data[\"metadatas\"]\n",
    "\n",
    "image_count = sum(1 for m in metas if m.get(\"type\") == \"image\")\n",
    "text_count  = sum(1 for m in metas if m.get(\"type\") == \"text\")\n",
    "\n",
    "print(f\"Total items: {len(metas)} | Images: {image_count} | Texts: {text_count}\")\n",
    "\n",
    "print(multi_col._embedding_function)\n",
    "\n",
    "query = np.array(Image.open(\"calico-beans.jpg\").convert(\"RGB\"))\n",
    "\n",
    "res = multi_col.query(\n",
    "    query_images=[query],\n",
    "    n_results=85,    # top-k results\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "print_multi_summary(res)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
