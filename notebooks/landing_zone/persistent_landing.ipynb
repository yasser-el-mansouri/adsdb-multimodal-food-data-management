{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5b7408",
   "metadata": {},
   "source": [
    "# Landing Zone — Persistent\n",
    "\n",
    "This notebook handles the **raw data organization** step for the Landing Zone of our data pipeline.  \n",
    "Its primary goal is to:\n",
    "\n",
    "1. **Ingest raw data** from temporal Landing Zone\n",
    "2. **Store the data** in MinIo by type of file, and apllying name convention\n",
    "3. **Delete the data** from temporal Landing Zone if it's needed\n",
    "4. **Making recipes indexes** for usage in trusted zone and for data governance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e4074",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import time\n",
    "import boto3\n",
    "import mimetypes\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import unquote\n",
    "from pathlib import PurePosixPath\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SRC_BUCKET      = \"landing-zone\"\n",
    "DEST_BUCKET     = SRC_BUCKET\n",
    "IMG_PREFIX      = \"persistent_landing/images\"\n",
    "DOC_PREFIX      = \"persistent_landing/documents\"\n",
    "HF_DATASET=os.getenv(\"HF_DATASET\")\n",
    "\n",
    "MINIO_USER=os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD=os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT=os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "DELETE_SOURCE_AFTER_COPY = True \n",
    "\n",
    "IMAGE_MIME_PREFIXES = (\"image/\",)\n",
    "IMAGE_EXTS = {\"jpg\", \"jpeg\", \"png\", \"gif\", \"webp\", \"bmp\", \"tiff\"}\n",
    "DOC_EXTS   = {\"json\", \"jsonl\", \"ndjson\"}\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\",\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c6790",
   "metadata": {},
   "source": [
    "This block loads environment variables and sets up a MinIO connection using **boto3**. It defines source and destination buckets, prefixes for images and documents, and basic file type filters for images and JSON files.\n",
    "\n",
    "The script also includes a flag to optionally delete source files after copying, supporting cleanup operations. Overall, this section prepares the environment and storage connection for later steps that organize and move files within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e9b14",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_ts() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n",
    "\n",
    "\n",
    "def guess_name_and_ext(key: str, head: dict) -> tuple[str, str]:\n",
    "    p = PurePosixPath(key)\n",
    "    name = p.name\n",
    "    base = p.stem or \"file\"\n",
    "    ext = p.suffix.lower().lstrip(\".\")\n",
    "\n",
    "    if not ext:\n",
    "        ctype = (head.get(\"ContentType\") or \"\").split(\";\")[0].strip().lower()\n",
    "        if ctype:\n",
    "            guess = mimetypes.guess_extension(ctype) or \"\"\n",
    "            ext = guess.lstrip(\".\")\n",
    "            if ext == \"jpe\":\n",
    "                ext = \"jpg\"\n",
    "    if ext == \"jpeg\":\n",
    "        ext = \"jpg\"\n",
    "    return base, ext or \"bin\"\n",
    "\n",
    "\n",
    "def is_image(head: dict, ext: str) -> bool:\n",
    "    ctype = (head.get(\"ContentType\") or \"\").lower()\n",
    "    return ctype.startswith(IMAGE_MIME_PREFIXES) or ext in IMAGE_EXTS\n",
    "\n",
    "\n",
    "def is_document_json(head: dict, ext: str) -> bool:\n",
    "    ctype = (head.get(\"ContentType\") or \"\").split(\";\")[0].strip().lower()\n",
    "    return ext in DOC_EXTS or ctype == \"application/json\"\n",
    "\n",
    "\n",
    "def sanitize_filename(s: str) -> str:\n",
    "    return re.sub(r\"[^\\w\\-.]+\", \"_\", s)\n",
    "\n",
    "\n",
    "def make_target_key(obj_type: str, dataset: str, ts: str, filename: str, ext: str, prefix: str) -> str:\n",
    "    filename = sanitize_filename(filename)\n",
    "    dataset  = sanitize_filename(dataset)\n",
    "    return f\"{prefix}/{obj_type}${dataset}${ts}${filename}.{ext}\"\n",
    "\n",
    "\n",
    "def copy_object(src_bucket: str, src_key: str, dst_bucket: str, dst_key: str, metadata: dict | None = None, content_type: str | None = None):\n",
    "    extra = {\"MetadataDirective\": \"REPLACE\"}\n",
    "    if metadata:\n",
    "        extra[\"Metadata\"] = metadata\n",
    "    if content_type:\n",
    "        extra[\"ContentType\"] = content_type\n",
    "\n",
    "    s3.copy_object(\n",
    "        CopySource={\"Bucket\": src_bucket, \"Key\": src_key},\n",
    "        Bucket=dst_bucket,\n",
    "        Key=dst_key,\n",
    "        **extra,\n",
    "    )\n",
    "\n",
    "\n",
    "def move_or_copy(src_bucket: str, src_key: str, dst_bucket: str, dst_key: str, **kwargs):\n",
    "    copy_object(src_bucket, src_key, dst_bucket, dst_key, **kwargs)\n",
    "    if DELETE_SOURCE_AFTER_COPY:\n",
    "        try:\n",
    "            s3.delete_object(Bucket=src_bucket, Key=src_key)\n",
    "        except ClientError as e:\n",
    "            print(f\"[WARN] failed to delete from origin {src_key}: {e}\")\n",
    "            \n",
    "            \n",
    "def atomic_write_json(path: str, data: dict):\n",
    "    tmp = f\"{path}.tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(data, fh, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def extract_recipe_ids(bucket: str, key: str) -> set[str]:\n",
    "    ids: set[str] = set()\n",
    "    ext = PurePosixPath(key).suffix.lower()\n",
    "\n",
    "    try:\n",
    "        if ext in (\".jsonl\", \".ndjson\"):\n",
    "            obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "            for line in obj[\"Body\"].iter_lines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    if isinstance(rec, dict) and \"id\" in rec:\n",
    "                        ids.add(str(rec[\"id\"]))\n",
    "                except Exception:\n",
    "                    continue\n",
    "            return ids\n",
    "\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj[\"Body\"].read()\n",
    "        try:\n",
    "            data = json.loads(body)\n",
    "            if isinstance(data, list):\n",
    "                for rec in data:\n",
    "                    if isinstance(rec, dict) and \"id\" in rec:\n",
    "                        ids.add(str(rec[\"id\"]))\n",
    "            elif isinstance(data, dict):\n",
    "                if \"id\" in data:\n",
    "                    ids.add(str(data[\"id\"]))\n",
    "                for v in data.values():\n",
    "                    if isinstance(v, list):\n",
    "                        for rec in v:\n",
    "                            if isinstance(rec, dict) and \"id\" in rec:\n",
    "                                ids.add(str(rec[\"id\"]))\n",
    "        except Exception:\n",
    "            pass\n",
    "    except ClientError:\n",
    "        pass\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa5254",
   "metadata": {},
   "source": [
    "This block defines utility functions for classifying, naming, and moving files between MinIO buckets.\n",
    "\n",
    "`utc_ts` generates a UTC timestamp used to version or label copied objects. `guess_name_and_ext` extracts or infers a file’s base name and extension from its key or content type, normalizing image formats like `.jpeg` to `.jpg`. The functions `is_image` and `is_document_json` classify files by checking MIME types and extensions.\n",
    "\n",
    "`sanitize_filename` cleans names to avoid invalid characters, while `make_target_key` builds standardized keys using the pattern `type$dataset$timestamp$name.extension`, ensuring unique and descriptive filenames.\n",
    "\n",
    "Finally, `copy_object` and `move_or_copy` handle the actual file transfer, replacing metadata if needed and optionally deleting the source file after copying, supporting a clean migration process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b515b",
   "metadata": {},
   "source": [
    "## 3. Group data by type, apply name convention and store on persistent zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05039bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "ing_ts = utc_ts()\n",
    "\n",
    "pages = paginator.paginate(Bucket=SRC_BUCKET)\n",
    "total = moved_img = moved_doc = skipped = 0\n",
    "\n",
    "images_index = {}\n",
    "IMAGE_INDEX_PATH = \"image_index.json\"\n",
    "\n",
    "recipes_index: dict[str, dict] = {}\n",
    "RECIPES_INDEX_PATH = \"recipes_index.json\"\n",
    "\n",
    "for page in pages:\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        total += 1\n",
    "\n",
    "        if key.endswith(\"/\") or key.startswith(\".\"):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            head = s3.head_object(Bucket=SRC_BUCKET, Key=key)\n",
    "        except ClientError as e:\n",
    "            print(f\"[WARN] head_object failed in {key}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        base, ext = guess_name_and_ext(key, head)\n",
    "\n",
    "        if is_image(head, ext):\n",
    "            dst_key = make_target_key(\"image\", HF_DATASET, ing_ts, base, ext, prefix=IMG_PREFIX)\n",
    "            move_or_copy(\n",
    "                SRC_BUCKET, key, DEST_BUCKET, dst_key,\n",
    "                metadata={\n",
    "                    \"src-bucket\": SRC_BUCKET,\n",
    "                    \"src-key\": key,\n",
    "                    \"dataset\": HF_DATASET,\n",
    "                    \"ingestion-ts\": ing_ts,\n",
    "                },\n",
    "                content_type=head.get(\"ContentType\"),\n",
    "            )\n",
    "            moved_img += 1\n",
    "            print(f\"[IMG] {key} -> s3://{DEST_BUCKET}/{dst_key}\")\n",
    "            \n",
    "            dst_name = PurePosixPath(dst_key).name\n",
    "            id_part = re.search(r'([A-Fa-f0-9]+[0-9]+)\\.[^.]+$', dst_name)\n",
    "            image_id = id_part.group(1) if id_part else dst_name\n",
    "\n",
    "            images_index[key] = {\n",
    "                \"id\": image_id,\n",
    "            }\n",
    "\n",
    "        elif is_document_json(head, ext):\n",
    "            dst_key = make_target_key(\"document\", HF_DATASET, ing_ts, base, ext, prefix=DOC_PREFIX)\n",
    "            move_or_copy(\n",
    "                SRC_BUCKET, key, DEST_BUCKET, dst_key,\n",
    "                metadata={\n",
    "                    \"src-bucket\": SRC_BUCKET,\n",
    "                    \"src-key\": key,\n",
    "                    \"dataset\": HF_DATASET,\n",
    "                    \"ingestion-ts\": ing_ts,\n",
    "                },\n",
    "                content_type=head.get(\"ContentType\") or \"application/json\",\n",
    "            )\n",
    "            moved_doc += 1\n",
    "            print(f\"[DOC] {key} -> s3://{DEST_BUCKET}/{dst_key}\")\n",
    "            doc_ids = extract_recipe_ids(DEST_BUCKET, dst_key)\n",
    "            \n",
    "            for rid in doc_ids:\n",
    "                entry = recipes_index.setdefault(rid, {\"docs\": []})\n",
    "                if dst_key not in entry[\"docs\"]:\n",
    "                    entry[\"docs\"].append(dst_key)\n",
    "\n",
    "            print(f\"[IDX] ids in doc: {len(doc_ids)}\")\n",
    "            \n",
    "        else:\n",
    "            skipped += 1\n",
    "            print(f\"[SKIP] {key} (ctype={head.get('ContentType')}, ext=.{ext})\")\n",
    "\n",
    "if images_index:\n",
    "    try:\n",
    "        atomic_write_json(IMAGE_INDEX_PATH, images_index)\n",
    "        print(f\"\\n[INDEX] indexed images: {len(images_index)} -> {IMAGE_INDEX_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to write local index {IMAGE_INDEX_PATH}: {e}\")\n",
    "\n",
    "if recipes_index:\n",
    "    for rid, info in recipes_index.items():\n",
    "        info[\"docs\"] = sorted(set(info.get(\"docs\", [])))\n",
    "    try:\n",
    "        atomic_write_json(RECIPES_INDEX_PATH, recipes_index)\n",
    "        print(f\"[INDEX] indexed recipes: {len(recipes_index)} -> {RECIPES_INDEX_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to write {RECIPES_INDEX_PATH}: {e}\")\n",
    "\n",
    "print(f\"\\n[STATS] total={total}  images={moved_img}  documents={moved_doc}  skipped={skipped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8027f6",
   "metadata": {},
   "source": [
    "This block scans all objects in the source MinIO bucket and organizes them into structured destinations based on file type.\n",
    "\n",
    "It uses a paginator to iterate through every stored object, skipping directories or hidden files. For each valid file, the script retrieves metadata, determines its type, and applies the standardized naming convention `type$dataset$timestamp$name.extension`.\n",
    "\n",
    "Image files are moved under the image prefix, and JSON documents under the document prefix — both enriched with metadata such as source path, dataset name, and ingestion timestamp. Unsupported or unrecognized files are skipped.\n",
    "\n",
    "Finally, summary statistics are printed, showing totals for processed, moved, and skipped files, providing a quick overview of the ingestion process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
