"""
Exploitation Zone - Documents Processing

This module handles the text processing step for the Exploitation Zone.
It generates embeddings from documents and stores them in ChromaDB.
"""

import os
import json
import gzip
from typing import Dict, List, Any, Iterable, Optional

import chromadb
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

from app.utils import (
    PipelineConfig, S3Client, Logger, PerformanceMonitor, 
    utc_timestamp, error_handler
)


class ExploitationDocumentsProcessor:
    """Processor for exploitation zone documents."""
    
    def __init__(self, config: PipelineConfig):
        """Initialize the processor."""
        self.config = config
        self.logger = Logger("exploitation_documents", config.get("monitoring.log_level", "INFO"))
        self.s3_client = S3Client(config)
        self.monitor = PerformanceMonitor(config)
        
        # Configuration
        self.src_bucket = config.get("storage.buckets.trusted_zone")
        self.src_prefix = config.get("storage.prefixes.trusted_documents")
        
        # ChromaDB configuration
        chromadb_config = config.get("chromadb", {})
        self.collection_name = chromadb_config.get("collection_name", "trusted_zone_documents")
        self.embedding_model = chromadb_config.get("embedding_model", "Qwen/Qwen3-Embedding-0.6B")
        self.metadata = chromadb_config.get("metadata", {})
        
        # Processing configuration
        self.batch_size = config.get("pipeline.batch_size", 256)
        self.persist_dir = config.get_env("CHROMA_PERSIST_DIR", "./chroma_exploitation")
        
        # Initialize ChromaDB
        self.chroma_client = PersistentClient(path=self.persist_dir)
        self.ef_text = SentenceTransformerEmbeddingFunction(model_name=self.embedding_model)
        self.recipes_col = self.chroma_client.get_or_create_collection(
            name=self.collection_name,
            embedding_function=self.ef_text,
            metadata=self.metadata
        )
        
        self.logger.info(f"ChromaDB directory: {self.persist_dir}")
    
    def list_all_recipe_keys(self, bucket: str, prefix: str) -> List[str]:
        """List all recipe file keys in bucket with prefix."""
        keys = []
        exts = (".jsonl",)
        
        paginator = self.s3_client.client.get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get("Contents", []):
                k = obj["Key"]
                if k.lower().endswith(exts):
                    keys.append(k)
                    self.logger.info(f"Found recipe file: {k}")
        
        return keys
    
    def stream_objects_lines(self, bucket: str, key: str) -> Iterable[str]:
        """Stream lines from S3 object."""
        obj = self.s3_client.get_object(Bucket=bucket, Key=key)
        raw = obj["Body"].read()
        
        if key.lower().endswith(".gz"):
            raw = gzip.decompress(raw)
        
        text = raw.decode("utf-8", errors="replace")
        if key.lower().endswith(".jsonl"):
            for line in text.splitlines():
                line = line.strip()
                if line:
                    yield line
    
    def normalize_recipe_json(self, rec: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Normalize recipe JSON for embedding."""
        rid = str(rec.get("id") or "").strip()
        if not rid:
            return None
        
        # Extract title
        title = (rec.get("title_text_clean") or
                rec.get("title__from_recipes_with_nutritional_info") or
                rec.get("title__from_layer1") or
                rec.get("title_text_raw") or "").strip()
        
        # Extract ingredients
        ing_clean = rec.get("ingredients_text_clean")
        if not ing_clean:
            ing_lists = (rec.get("ingredients__from_recipes_with_nutritional_info") or
                        rec.get("ingredients__from_layer1") or
                        rec.get("ingredients__from_det_ingrs") or [])
            ing_clean = " ".join(x.get("text", "").strip() for x in ing_lists if isinstance(x, dict))
        
        # Extract instructions
        instr_clean = rec.get("instructions_text_clean")
        if not instr_clean:
            instr_lists = (rec.get("instructions__from_recipes_with_nutritional_info") or
                          rec.get("instructions__from_layer1") or [])
            instr_clean = " ".join(x.get("text", "").strip() for x in instr_lists if isinstance(x, dict))
        
        # Build document text
        parts = []
        if title:
            parts.append(f"title: {title}")
        if ing_clean:
            parts.append(f"ingredients: {ing_clean}")
        if instr_clean:
            parts.append(f"instructions: {instr_clean}")
        
        document = "\n".join(parts).strip()
        if not document:
            return None
        
        # Extract FSA lights
        lights = rec.get("fsa_lights_per100g__from_recipes_with_nutritional_info") or {}
        
        # Build metadata
        meta = {
            "id": rid,
            "title": title,
            "has_nutrition": bool(rec.get("nutrition_normalized")),
            "source_bucket": "trusted-zone",
        }
        
        # Add FSA values if they're not None
        if lights.get("fat") is not None:
            meta["fsa_fat"] = lights.get("fat")
        if lights.get("salt") is not None:
            meta["fsa_salt"] = lights.get("salt")
        if lights.get("saturates") is not None:
            meta["fsa_saturates"] = lights.get("saturates")
        if lights.get("sugars") is not None:
            meta["fsa_sugars"] = lights.get("sugars")
        
        return {"id": rid, "document": document, "metadata": meta}
    
    def ingest_recipes(self, bucket: str, prefix: str, batch: int = None) -> Dict[str, Any]:
        """Ingest recipes from S3 into ChromaDB."""
        if batch is None:
            batch = self.batch_size
        
        buf_ids, buf_docs, buf_meta = [], [], []
        n_ok, n_bad = 0, 0
        
        keys = self.list_all_recipe_keys(bucket, prefix)
        
        for key in keys:
            self.logger.info(f"Processing file: {key}")
            
            for payload in self.stream_objects_lines(bucket, key):
                try:
                    data = json.loads(payload)
                    if isinstance(data, dict):
                        items = [data]
                    elif isinstance(data, list):
                        items = data
                    else:
                        raise ValueError("JSON root is not a dict or list")
                    
                    for rec in items:
                        if not isinstance(rec, dict):
                            n_bad += 1
                            continue
                        
                        pack = self.normalize_recipe_json(rec)
                        if not pack:
                            n_bad += 1
                            continue
                        
                        buf_ids.append(pack["id"])
                        buf_docs.append(pack["document"])
                        buf_meta.append(pack["metadata"])
                        n_ok += 1
                        
                        if len(buf_ids) >= batch:
                            self.recipes_col.add(
                                ids=buf_ids, 
                                documents=buf_docs, 
                                metadatas=buf_meta
                            )
                            self.logger.info(f"Added batch of {len(buf_ids)} recipes to ChromaDB")
                            buf_ids, buf_docs, buf_meta = [], [], []
                
                except Exception as e:
                    self.logger.warning(f"Error processing payload: {e}")
                    n_bad += 1
        
        # Add remaining items
        if buf_ids:
            self.recipes_col.add(ids=buf_ids, documents=buf_docs, metadatas=buf_meta)
            self.logger.info(f"Added final batch of {len(buf_ids)} recipes to ChromaDB")
        
        self.logger.info(f"Ingestion completed. OK: {n_ok} | discarded: {n_bad}")
        
        return {
            "processed_records": n_ok,
            "discarded_records": n_bad,
            "total_files": len(keys)
        }
    
    def test_query(self, query: str = "honey sriracha chicken wings", n_results: int = 1) -> Dict[str, Any]:
        """Test query against the collection."""
        try:
            res = self.recipes_col.query(
                query_texts=[query],
                n_results=n_results,
                include=["metadatas", "distances", "documents"],
            )
            
            self.logger.info(f"Query: {query}")
            self.logger.info(f"Results: {json.dumps(res, indent=2, ensure_ascii=False)}")
            
            return res
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            return {}
    
    def process(self) -> Dict[str, Any]:
        """Main processing method."""
        self.monitor.start()
        
        try:
            with error_handler(self.logger, "exploitation_documents_processing"):
                # Ingest recipes
                ingestion_stats = self.ingest_recipes(self.src_bucket, self.src_prefix)
                
                # Test query
                query_results = self.test_query()
                
                metrics = self.monitor.stop()
                metrics.update({
                    **ingestion_stats,
                    "timestamp": utc_timestamp(),
                    "collection_name": self.collection_name,
                    "embedding_model": self.embedding_model
                })
                
                self.logger.info(f"Exploitation documents processing completed: {metrics}")
                return metrics
        
        except Exception as e:
            self.logger.error(f"Exploitation documents processing failed: {e}")
            raise


def main():
    """Main entry point for exploitation documents processing."""
    config = PipelineConfig()
    processor = ExploitationDocumentsProcessor(config)
    
    try:
        result = processor.process()
        print(f"‚úÖ Exploitation documents processing completed successfully")
        print(f"üìä Metrics: {result}")
    except Exception as e:
        print(f"‚ùå Exploitation documents processing failed: {e}")
        raise


if __name__ == "__main__":
    main()
