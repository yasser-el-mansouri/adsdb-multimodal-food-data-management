"""
Exploitation Zone - Images Processing

This module handles the image processing step for the Exploitation Zone.
It generates embeddings from images and stores them in ChromaDB.
"""

import io
import os
from typing import Any, Dict, Iterable, List, Optional

import chromadb
import numpy as np
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction
from PIL import Image

# Import shared utilities
from app.utils.shared import Logger, PipelineConfig, S3Client, error_handler, utc_timestamp


class ExploitationImagesProcessor:
    """Processor for exploitation zone images."""

    def __init__(self, config: PipelineConfig):
        """Initialize the processor."""
        self.config = config
        self.logger = Logger("exploitation_images", config.get("monitoring.log_level", "INFO"))
        self.s3_client = S3Client(config)

        # Configuration
        self.src_bucket = config.get("storage.buckets.trusted_zone")
        self.src_prefix = config.get("storage.prefixes.trusted_images")

        # ChromaDB configuration
        chromadb_config = config.get("chromadb_images", {})
        self.collection_name = chromadb_config.get("collection_name", "exploitation_images")
        self.embedding_model = chromadb_config.get("embedding_model", "OpenCLIP")
        self.metadata = chromadb_config.get("metadata", {})
        self.persist_dir = chromadb_config.get(
            "persist_dir", "app/zones/exploitation_zone/chroma_images"
        )

        chromadb_config_multi = config.get("chromadb_multimodal", {})
        self.collection_name_multi = chromadb_config_multi.get("collection_name", "exploitation_multimodal")
        self.embedding_model_multi = chromadb_config_multi.get("embedding_model", "OpenCLIP")
        self.metadata_multi = chromadb_config_multi.get("metadata", {})
        self.persist_dir_multi = chromadb_config_multi.get(
            "persist_dir", "app/zones/exploitation_zone/chroma_multimodal"
        )

        # Processing configuration
        self.batch_size = config.get("pipeline.batch_size", 128)

        # Initialize ChromaDB client and embedding function
        self.chroma_client = PersistentClient(path=self.persist_dir)
        self.ef_images = OpenCLIPEmbeddingFunction(model_name=self.embedding_model)
        self.ef_multimodal = OpenCLIPEmbeddingFunction(model_name=self.embedding_model_multi)

        # Initialize ChromaDB collection
        collection_kwargs = {"name": self.collection_name, "embedding_function": self.ef_images}
        multi_collection_kwargs = {"name": self.collection_name_multi, "embedding_function": self.ef_multimodal}

        # Only add metadata if it's not empty
        if self.metadata:
            collection_kwargs["metadata"] = self.metadata
        if self.metadata_multi:
            multi_collection_kwargs["metadata"] = self.metadata_multi

        self.images_col = self.chroma_client.get_or_create_collection(**collection_kwargs)
        self.multi_col = self.chroma_client.get_or_create_collection(**multi_collection_kwargs)

        self.logger.info(f"ChromaDB directory: {self.persist_dir}")

    def list_all_image_keys(self, bucket: str, prefix: str) -> List[str]:
        """List all image file keys in bucket with prefix."""
        keys = []
        exts = (".jpg", ".jpeg", ".png", ".bmp", ".tiff")

        paginator = self.s3_client.client.get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get("Contents", []):
                k = obj["Key"]
                if k.lower().endswith(exts):
                    keys.append(k)
                    self.logger.info(f"Found image file: {k}")

        return keys

    def get_image_from_s3(self, bucket: str, key: str) -> np.ndarray:
        """Get image from S3 and convert to numpy array."""
        try:
            obj = self.s3_client.get_object(bucket=bucket, key=key)
            image_data = obj["Body"].read()

            # Convert to PIL Image and then to numpy array
            image = Image.open(io.BytesIO(image_data)).convert("RGB")
            return np.array(image)

        except Exception as e:
            self.logger.warning(f"Failed to load image {key}: {e}")
            # Return a fallback white image
            return np.full((224, 224, 3), 255, dtype=np.uint8)

    def normalize_image_key(self, key: str) -> str:
        """Normalize image key for ChromaDB ID."""
        # Replace slashes with double underscores to avoid issues
        return key.replace("/", "__")

    def ingest_images(self, bucket: str, prefix: str, batch: int = None) -> Dict[str, Any]:
        """Ingest images from S3 into ChromaDB."""
        if batch is None:
            batch = self.batch_size

        buf_ids, buf_images, buf_meta = [], [], []
        n_ok, n_bad = 0, 0

        keys = self.list_all_image_keys(bucket, prefix)

        for i, key in enumerate(keys, 1):
            self.logger.info(f"Processing image {i}/{len(keys)}: {key}")

            try:
                # Load image
                image_array = self.get_image_from_s3(bucket, key)

                # Normalize key for ChromaDB ID
                normalized_id = self.normalize_image_key(key)

                # Build metadata
                metadata = {
                    "type": "image",
                    "object_key": key,
                    "bucket": bucket,
                    "source_prefix": prefix,
                    "image_height": int(image_array.shape[0]),
                    "image_width": int(image_array.shape[1]),
                    "image_channels": int(image_array.shape[2]),
                    "processed_at": utc_timestamp(),
                }

                buf_ids.append(normalized_id)
                buf_images.append(image_array)
                buf_meta.append(metadata)
                n_ok += 1

                # Add batch to ChromaDB when batch size is reached
                if len(buf_ids) >= batch:
                    self.images_col.add(ids=buf_ids, images=buf_images, metadatas=buf_meta)
                    self.multi_col.add(ids=buf_ids, images=buf_images, metadatas=buf_meta)
                    self.logger.info(f"Added batch of {len(buf_ids)} images to ChromaDB")
                    buf_ids, buf_images, buf_meta = [], [], []

            except Exception as e:
                self.logger.warning(f"Error processing image {key}: {e}")
                n_bad += 1

        # Add remaining items
        if buf_ids:
            self.images_col.add(ids=buf_ids, images=buf_images, metadatas=buf_meta)
            self.multi_col.add(ids=buf_ids, images=buf_images, metadatas=buf_meta)
            self.logger.info(f"Added final batch of {len(buf_ids)} images to ChromaDB")

        self.logger.info(f"Ingestion completed. OK: {n_ok} | failed: {n_bad}")

        return {"processed_images": n_ok, "failed_images": n_bad, "total_files": len(keys)}

    def test_query(
        self, query: str = "pasta with tomato and basil", n_results: int = 5
    ) -> Dict[str, Any]:
        """Test query against the collection."""
        try:
            res = self.images_col.query(
                query_texts=[query], n_results=n_results, include=["metadatas", "distances"]
            )

            self.logger.info(f"Query: {query}")
            self.logger.info(f"Found {len(res['ids'][0])} results")

            return res
        except Exception as e:
            self.logger.error(f"Query failed: {e}")
            return {}

    def process(self) -> Dict[str, Any]:
        """Main processing method."""
        try:
            with error_handler(self.logger, "exploitation_images_processing"):
                # Ingest images
                ingestion_stats = self.ingest_images(self.src_bucket, self.src_prefix)

                # Test query
                query_results = self.test_query()

                result = {
                    **ingestion_stats,
                    "timestamp": utc_timestamp(),
                    "collection_name": self.collection_name,
                    "embedding_model": self.embedding_model,
                }

                self.logger.info(f"Exploitation images processing completed: {result}")
                return result

        except Exception as e:
            self.logger.error(f"Exploitation images processing failed: {e}")
            raise


def main():
    """Main entry point for exploitation images processing."""
    config = PipelineConfig()
    processor = ExploitationImagesProcessor(config)

    try:
        result = processor.process()
        print(f"‚úÖ Exploitation images processing completed successfully")
        print(f"üìä Metrics: {result}")
    except Exception as e:
        print(f"‚ùå Exploitation images processing failed: {e}")
        raise


if __name__ == "__main__":
    main()
