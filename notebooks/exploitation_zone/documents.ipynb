{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b23e713",
   "metadata": {},
   "source": [
    "# Exploitation Zone ‚Äî Text Processing\n",
    "\n",
    "This notebook handles the **text processing** step for the Exploitation Zone of our data pipeline.  \n",
    "Its primary goal is to:\n",
    "\n",
    "1. **Load documents** from the Trusted Zone\n",
    "2. **Generate embeddings** from the documents in the Trusted Zone\n",
    "3. **Store the embeddings** in ChromaDB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87d825",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6082552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, gzip\n",
    "from datetime import datetime, timezone\n",
    "from typing import Iterable, List\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction, OpenCLIPEmbeddingFunction\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MINIO_USER     = os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "TRUST_BUCKET         = \"trusted-zone\"\n",
    "TRUST_RECIPES_PREFIX = \"documents\"\n",
    "\n",
    "PERSIST_DIR = os.getenv(\"CHROMA_PERSIST_DIR\")\n",
    "\n",
    "BATCH = 256\n",
    "\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"}),\n",
    ")\n",
    "\n",
    "chroma_client = PersistentClient(path=PERSIST_DIR)\n",
    "ef_text = SentenceTransformerEmbeddingFunction(model_name=\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "ef_multi = OpenCLIPEmbeddingFunction()\n",
    "\n",
    "docu_name = \"trusted_zone_documents\"\n",
    "multi_name = \"trusted_zone_multimodal\"\n",
    "\n",
    "recipes_col = chroma_client.get_or_create_collection(\n",
    "    name=docu_name,\n",
    "    embedding_function=ef_text,\n",
    "    metadata={\"modality\": \"text\", \"model\": \"Qwen/Qwen3-Embedding-0.6B\", \"source\": \"minio\"},\n",
    ")\n",
    "\n",
    "multi_col = chroma_client.get_or_create_collection(\n",
    "    name=multi_name,\n",
    "    embedding_function=ef_multi,\n",
    "    metadata={\"modality\": \"multimodal\", \"model\": \"OpenCLIP\", \"source\": \"minio\"}\n",
    ")\n",
    "print(\"‚úÖ Chroma DB directory:\", PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361ba5b",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e67655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_ts() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n",
    "\n",
    "\n",
    "def list_all_recipe_keys(bucket: str, prefix: str) -> List[str]:\n",
    "    keys, token = [], None\n",
    "    exts = (\".jsonl\")\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix, \"MaxKeys\": 1000}\n",
    "        if token: kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            k = obj[\"Key\"]\n",
    "            if k.lower().endswith(exts):\n",
    "                keys.append(k)\n",
    "                print(f\"Found recipe file: {k}\")\n",
    "        token = resp.get(\"NextContinuationToken\")\n",
    "        if not token:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "\n",
    "def stream_objects_lines(bucket: str, key: str) -> Iterable[str]:\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    raw = obj[\"Body\"].read()\n",
    "    if key.lower().endswith(\".gz\"):\n",
    "        raw = gzip.decompress(raw)\n",
    "\n",
    "    text = raw.decode(\"utf-8\", errors=\"replace\")\n",
    "    if key.lower().endswith(\".jsonl\"):\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield line\n",
    "\n",
    "\n",
    "def normalize_recipe_json(rec: dict):\n",
    "    rid = str(rec.get(\"id\") or \"\").strip()\n",
    "    if not rid:\n",
    "        return None\n",
    "\n",
    "    title = (rec.get(\"title_text_clean\")\n",
    "             or rec.get(\"title__from_recipes_with_nutritional_info\")\n",
    "             or rec.get(\"title__from_layer1\")\n",
    "             or rec.get(\"title_text_raw\")\n",
    "             or \"\").strip()\n",
    "\n",
    "    ing_clean = rec.get(\"ingredients_text_clean\")\n",
    "    if not ing_clean:\n",
    "        ing_lists = (rec.get(\"ingredients__from_recipes_with_nutritional_info\")\n",
    "                     or rec.get(\"ingredients__from_layer1\")\n",
    "                     or rec.get(\"ingredients__from_det_ingrs\")\n",
    "                     or [])\n",
    "        ing_clean = \" \".join(x.get(\"text\",\"\").strip() for x in ing_lists if isinstance(x, dict))\n",
    "\n",
    "    instr_clean = rec.get(\"instructions_text_clean\")\n",
    "    if not instr_clean:\n",
    "        instr_lists = (rec.get(\"instructions__from_recipes_with_nutritional_info\")\n",
    "                       or rec.get(\"instructions__from_layer1\")\n",
    "                       or [])\n",
    "        instr_clean = \" \".join(x.get(\"text\",\"\").strip() for x in instr_lists if isinstance(x, dict))\n",
    "\n",
    "    parts = []\n",
    "    if title: parts.append(f\"title: {title}\")\n",
    "    if ing_clean: parts.append(f\"ingredients: {ing_clean}\")\n",
    "    if instr_clean: parts.append(f\"instructions: {instr_clean}\")\n",
    "    document = \"\\n\".join(parts).strip()\n",
    "    if not document:\n",
    "        return None\n",
    "\n",
    "    lights = rec.get(\"fsa_lights_per100g__from_recipes_with_nutritional_info\") or {}\n",
    "    \n",
    "    # Build metadata and filter out None values\n",
    "    meta = {\n",
    "        \"id\": rid,\n",
    "        \"title\": title,\n",
    "        \"has_nutrition\": bool(rec.get(\"nutrition_normalized\")),\n",
    "        \"source_bucket\": \"trusted-zone\",\n",
    "    }\n",
    "    \n",
    "    # Only add FSA values if they're not None\n",
    "    if lights.get(\"fat\") is not None:\n",
    "        meta[\"fsa_fat\"] = lights.get(\"fat\")\n",
    "    if lights.get(\"salt\") is not None:\n",
    "        meta[\"fsa_salt\"] = lights.get(\"salt\")\n",
    "    if lights.get(\"saturates\") is not None:\n",
    "        meta[\"fsa_saturates\"] = lights.get(\"saturates\")\n",
    "    if lights.get(\"sugars\") is not None:\n",
    "        meta[\"fsa_sugars\"] = lights.get(\"sugars\")\n",
    "\n",
    "    return {\"id\": rid, \"document\": document, \"metadata\": meta}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb8999",
   "metadata": {},
   "source": [
    "This block defines utility functions for listing, streaming, and normalizing recipe data stored as JSONL objects in S3 buckets.\n",
    "\n",
    "`utc_ts` generates a UTC timestamp in ISO-like format, used for consistent logging or versioning.\n",
    "`list_all_recipe_keys` iterates through S3 objects under a given bucket and prefix, paginating with continuation tokens and collecting all keys ending in `.jsonl`, printing each recipe file found.\n",
    "\n",
    "`stream_objects_lines` retrieves an object‚Äôs raw bytes from S3, automatically decompressing `.gz` files when needed. It decodes the text as UTF-8 and yields non-empty lines from `.jsonl` files, allowing recipes to be processed line by line without loading entire datasets into memory.\n",
    "\n",
    "`normalize_recipe_json` cleans and standardizes raw recipe dictionaries. It extracts core fields such as `id`, `title`, `ingredients`, and `instructions` from various possible source attributes, consolidating them into a single coherent text document. The function also constructs a metadata dictionary containing identifiers, nutrition flags, and FSA nutritional ‚Äútraffic light‚Äù values (fat, salt, saturates, sugars), tagging the source bucket as `\"trusted-zone\"`.\n",
    "\n",
    "Together, these functions support scalable ingestion and normalization of recipe data, preparing it for indexing, analysis, or embedding downstream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939603a",
   "metadata": {},
   "source": [
    "## 3. Store rececipes ingredients, instructions and metadata as embeddings in chormaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_recipes(bucket: str, prefix: str, batch: int = BATCH, collection = None):\n",
    "    buf_ids, buf_docs, buf_meta = [], [], []\n",
    "    n_ok, n_bad = 0, 0\n",
    "\n",
    "    keys = list_all_recipe_keys(bucket, prefix)\n",
    "    for key in keys:\n",
    "        for payload in stream_objects_lines(bucket, key):\n",
    "            try:\n",
    "                data = json.loads(payload)\n",
    "                if isinstance(data, dict):\n",
    "                    items = [data]\n",
    "                elif isinstance(data, list):\n",
    "                    items = data\n",
    "                else:\n",
    "                    raise ValueError(\"JSON root is not a dict or list\")\n",
    "\n",
    "                for rec in items:\n",
    "                    if not isinstance(rec, dict):\n",
    "                        n_bad += 1\n",
    "                        continue\n",
    "                    pack = normalize_recipe_json(rec)\n",
    "                    if not pack:\n",
    "                        n_bad += 1\n",
    "                        continue\n",
    "\n",
    "                    buf_ids.append(pack[\"id\"])\n",
    "                    buf_docs.append(pack[\"document\"])\n",
    "                    buf_meta.append({\"type\": \"text\"} | pack[\"metadata\"])\n",
    "                    n_ok += 1\n",
    "\n",
    "                    if len(buf_ids) >= batch:\n",
    "                        collection.add(ids=buf_ids, documents=buf_docs, metadatas=buf_meta)\n",
    "                        buf_ids, buf_docs, buf_meta = [], [], []\n",
    "\n",
    "            except Exception as e:\n",
    "                n_bad += 1\n",
    "\n",
    "    if buf_ids:\n",
    "        collection.add(ids=buf_ids, documents=buf_docs, metadatas=buf_meta)\n",
    "\n",
    "    print(f\"‚úÖ Ingestion completed. OK: {n_ok}  |  discarded: {n_bad}\")\n",
    "#Have to separate these calls to avoid issues with caching\n",
    "ingest_recipes(TRUST_BUCKET, TRUST_RECIPES_PREFIX, batch=BATCH, collection=recipes_col)\n",
    "ingest_recipes(TRUST_BUCKET, TRUST_RECIPES_PREFIX, batch=BATCH, collection=multi_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"honey sriracha chicken wings\"\n",
    "res = recipes_col.query(\n",
    "    query_texts=[query],\n",
    "    n_results=1,\n",
    "    include=[\"metadatas\", \"distances\", \"documents\"],\n",
    ")\n",
    "print(\"üîé Query:\", query)\n",
    "print(json.dumps(res, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_recipe(res):\n",
    "    best_doc = res[\"documents\"][0][0]\n",
    "    meta = res[\"metadatas\"][0][0]\n",
    "    dist = float(res[\"distances\"][0][0])\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "\n",
    "    fsa_badge = {\n",
    "        \"green\": \"üü¢\",\n",
    "        \"orange\": \"üü†\",\n",
    "        \"red\": \"üî¥\",\n",
    "        None: \"‚ö™\",\n",
    "    }\n",
    "    print(\"üèÜ Closest Match\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ID: {meta.get('id')}\")\n",
    "    print(f\"Title: {str(meta.get('title','')).strip().capitalize()}\")\n",
    "    print(f\"Distance: {dist:.3f}  |  Similarity: {sim:.3f}\")\n",
    "    print(\"FSA Lights ‚Üí \"\n",
    "          f\"fat:{fsa_badge.get(meta.get('fsa_fat'))}  \"\n",
    "          f\"salt:{fsa_badge.get(meta.get('fsa_salt'))}  \"\n",
    "          f\"saturates:{fsa_badge.get(meta.get('fsa_saturates'))}  \"\n",
    "          f\"sugars:{fsa_badge.get(meta.get('fsa_sugars'))}\")\n",
    "    print(\"\\n--- Recipe ---\")\n",
    "\n",
    "    for line in best_doc.split(\"\\n\"):\n",
    "        if line.startswith(\"title:\"):\n",
    "            print(f\"\\nüßÅ {line.replace('title:', '').strip().capitalize()}\")\n",
    "        elif line.startswith(\"ingredients:\"):\n",
    "            print(\"\\nü•£ Ingredients:\")\n",
    "            ings = line.replace(\"ingredients:\", \"\").strip()\n",
    "            print(\"   \" + ings.replace(\"  \", \" \"))\n",
    "        elif line.startswith(\"instructions:\"):\n",
    "            print(\"\\nüë©‚Äçüç≥ Instructions:\")\n",
    "            instr = line.replace(\"instructions:\", \"\").strip()\n",
    "            print(\"   \" + instr.replace(\"  \", \" \"))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "show_recipe(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294235e8",
   "metadata": {},
   "source": [
    "\n",
    "This block ingests recipe data from the trusted S3 bucket, normalizes it, and indexes it into a ChromaDB collection for semantic search.\n",
    "\n",
    "`ingest_recipes` scans all `.jsonl` recipe files under the specified prefix, streaming each line directly from S3. It parses each JSON payload, cleans and standardizes its structure through `normalize_recipe_json`, and batches valid records into the vector database. Invalid or incomplete entries are counted and skipped, while successful ones are tracked for reporting.\n",
    "\n",
    "After ingestion, a sample query demonstrates text-to-recipe retrieval using semantic similarity. The `show_recipe` function formats the best match, displaying its title, FSA nutrition badges, and cleaned ingredients and instructions in a readable format.\n",
    "\n",
    "Together, these routines implement an end-to-end pipeline for structured recipe ingestion, embedding, and retrieval from the trusted data zone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
