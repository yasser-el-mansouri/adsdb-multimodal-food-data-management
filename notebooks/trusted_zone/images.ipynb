{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d08d1e",
   "metadata": {},
   "source": [
    "# Trusted Zone â€” Images Processing\n",
    "\n",
    "This notebook handles the **image processing** step for the Trusted Zone of our data pipeline.  \n",
    "Its primary goal is to:\n",
    "\n",
    "1. **Extract recipe IDs** from image filenames in the Formatted Zone\n",
    "2. **Identify which recipes have images** and build a mapping\n",
    "3. **Copy filtered images** to the Trusted Zone (only images with valid recipes)\n",
    "4. **Generate a recipe IDs file** for the documents processing step\n",
    "\n",
    "This notebook works in conjunction with `documents.ipynb` to ensure data integrity in the Trusted Zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2344e",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3422ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, json, re\n",
    "from pathlib import PurePosixPath\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Set, Iterable\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# S3 / MinIO Configuration\n",
    "MINIO_USER     = os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
    ")\n",
    "\n",
    "# Paths and Buckets\n",
    "FORM_BUCKET         = \"formatted-zone\"\n",
    "FORM_IMAGES_PREFIX  = \"images\"\n",
    "\n",
    "TRUST_BUCKET        = \"trusted-zone\"\n",
    "TRUST_IMAGES_PREFIX = \"images\"\n",
    "TRUST_REPORT_PREFIX = \"reports\"\n",
    "\n",
    "# Output file for documents processing\n",
    "RECIPE_IDS_FILE = \"recipe_ids_with_images.json\"\n",
    "\n",
    "# Behavior flags\n",
    "DRY_RUN   = False\n",
    "OVERWRITE = True\n",
    "\n",
    "def utc_ts():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H-%M-%SZ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04d2eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quality & normalization config ---\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "# Target canonical spec for Trusted Zone images\n",
    "TARGET_SIZE = (512, 512)         # WxH, letterboxed\n",
    "TARGET_MODE = \"RGB\"              # normalize mode\n",
    "TARGET_FMT  = \"JPEG\"             # write as .jpg\n",
    "TARGET_QUALITY = 90\n",
    "\n",
    "# Basic quality thresholds\n",
    "MIN_W, MIN_H = 128, 128          # remove tiny images\n",
    "MIN_ASPECT, MAX_ASPECT = 0.5, 3.0  # w/h range guard\n",
    "\n",
    "# blur screening using OpenCV (auto-off if not installed)\n",
    "try:\n",
    "    import cv2\n",
    "    CV2_AVAILABLE = True\n",
    "    BLUR_VARLAP_MIN = 50.0       # tune if needed\n",
    "except Exception:\n",
    "    CV2_AVAILABLE = False\n",
    "    BLUR_VARLAP_MIN = None\n",
    "\n",
    "# Near-duplicate removal (per recipe) via perceptual hash\n",
    "try:\n",
    "    import imagehash\n",
    "    DEDUPE_PER_RECIPE = True\n",
    "except Exception:\n",
    "    imagehash = None\n",
    "    DEDUPE_PER_RECIPE = False\n",
    "\n",
    "def compute_metrics(img: Image.Image) -> dict:\n",
    "    \"\"\"Return dict with width/height, aspect, (optional) blur metric.\"\"\"\n",
    "    w, h = img.size\n",
    "    aspect = (w / h) if h else 0\n",
    "    metrics = {\"w\": w, \"h\": h, \"aspect\": float(aspect)}\n",
    "    if CV2_AVAILABLE:\n",
    "        gray = cv2.cvtColor(np.array(img.convert(\"RGB\")), cv2.COLOR_RGB2GRAY)\n",
    "        metrics[\"blur_varlap\"] = float(cv2.Laplacian(gray, cv2.CV_64F).var())\n",
    "    return metrics\n",
    "\n",
    "def normalize_image(img: Image.Image) -> bytes:\n",
    "    \"\"\"Convert to TARGET_MODE, letterbox to TARGET_SIZE, write JPEG bytes.\"\"\"\n",
    "    img_rgb = img.convert(TARGET_MODE)\n",
    "    # Letterbox to keep aspect ratio\n",
    "    img_fit = ImageOps.pad(img_rgb, TARGET_SIZE, method=Image.BICUBIC, color=None, centering=(0.5, 0.5))\n",
    "    buf = io.BytesIO()\n",
    "    img_fit.save(buf, format=TARGET_FMT, quality=TARGET_QUALITY, optimize=True)\n",
    "    return buf.getvalue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cae1e",
   "metadata": {},
   "source": [
    "## 2. S3 Helper Functions\n",
    "\n",
    "These utility functions provide a clean interface for S3 operations, handling common patterns like listing objects, checking existence, and copying files between buckets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92e69fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_list_keys(bucket: str, prefix: str) -> Iterable[str]:\n",
    "    \"\"\"List all object keys in a bucket with the given prefix.\"\"\"\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []) or []:\n",
    "            key = obj[\"Key\"]\n",
    "            if not key.endswith(\"/\"):\n",
    "                yield key\n",
    "\n",
    "def s3_head(bucket: str, key: str):\n",
    "    \"\"\"Get object metadata, return None if not found.\"\"\"\n",
    "    try:\n",
    "        return s3.head_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        if e.response.get(\"Error\", {}).get(\"Code\") in (\"404\", \"NoSuchKey\", \"NotFound\"):\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "def s3_copy_object(src_bucket: str, src_key: str, dst_bucket: str, dst_key: str, overwrite: bool = True):\n",
    "    \"\"\"Copy an object between buckets with optional overwrite control.\"\"\"\n",
    "    if not overwrite and s3_head(dst_bucket, dst_key) is not None:\n",
    "        return \"skip-exists\"\n",
    "    return s3.copy_object(\n",
    "        Bucket=dst_bucket,\n",
    "        Key=dst_key,\n",
    "        CopySource={\"Bucket\": src_bucket, \"Key\": src_key},\n",
    "        MetadataDirective=\"COPY\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6326c57",
   "metadata": {},
   "source": [
    "## 3. Extract Recipe IDs from Image Filenames\n",
    "\n",
    "Each image stored in the Formatted Zone follows a structured naming convention that encodes metadata, including the recipe identifier.  \n",
    "The general pattern is:\n",
    "\n",
    "**fileType$dataSource$ingestionTimestamp$hash__recipeId_positionOnImagesUrlArrayFromLayer2.extension**\n",
    "\n",
    "From these filenames, we extract the `recipeId` using a regular expression.  \n",
    "This allows us to associate every image with its corresponding recipe entry, even when multiple images exist for the same recipe.  \n",
    "The result of this step is two structures:\n",
    "\n",
    "- `img_ids`: a set of **unique recipe IDs** that have at least one image.\n",
    "- `id_to_imgkeys`: a dictionary mapping each `recipeId` to **all its image keys** (to preserve one-to-many relationships).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1bdba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting recipe IDs from image filenames...\n",
      "[INFO] scanned image keys: 50\n",
      "[INFO] unique recipeIds with images: 35\n",
      "[INFO] total image files matched to recipeIds: 50\n"
     ]
    }
   ],
   "source": [
    "# Regular expression to extract recipe ID from image filenames\n",
    "# Recognizes names like:\n",
    "#   images/type$src$ts$hash__000018c8a5_0.jpg\n",
    "#   images/type$src$ts$hash__abcd_ef-12_3.JPEG\n",
    "# ID part: letters, digits, underscore, dash\n",
    "ID_REGEX = re.compile(\n",
    "    r\"__([A-Za-z0-9_\\-]+)_(\\d+)\\.(?:jpe?g|png|webp|gif|bmp|tiff)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def recipe_id_from_image_key(key: str) -> str | None:\n",
    "    \"\"\"Extract recipe ID from an image S3 key.\"\"\"\n",
    "    name = PurePosixPath(key).name\n",
    "    m = ID_REGEX.search(name)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "print(\"Extracting recipe IDs from image filenames...\")\n",
    "\n",
    "img_ids: Set[str] = set()                    # unique IDs (for filtering)\n",
    "id_to_imgkeys: Dict[str, List[str]] = {}     # ALL images per ID\n",
    "\n",
    "count_keys = 0\n",
    "for key in s3_list_keys(FORM_BUCKET, FORM_IMAGES_PREFIX + \"/\"):\n",
    "    count_keys += 1\n",
    "    rid = recipe_id_from_image_key(key)\n",
    "    if not rid:\n",
    "        continue\n",
    "    img_ids.add(rid)\n",
    "    id_to_imgkeys.setdefault(rid, []).append(key)\n",
    "\n",
    "# Make copies deterministic (optional)\n",
    "for rid in id_to_imgkeys:\n",
    "    id_to_imgkeys[rid].sort()\n",
    "\n",
    "total_images = sum(len(v) for v in id_to_imgkeys.values())\n",
    "print(f\"[INFO] scanned image keys: {count_keys}\")\n",
    "print(f\"[INFO] unique recipeIds with images: {len(img_ids)}\")\n",
    "print(f\"[INFO] total image files matched to recipeIds: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7256b8",
   "metadata": {},
   "source": [
    "## 4. Save Recipe IDs for Documents Processing\n",
    "\n",
    "We save the extracted recipe IDs to a JSON file that will be used by the `documents.ipynb` notebook to filter the recipe documents. This creates a clean separation between image and document processing while maintaining the necessary coupling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d8dce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved recipe IDs to recipe_ids_with_images.json\n",
      "[INFO] 35 recipe IDs will be used for document filtering\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for documents processing\n",
    "recipe_ids_data = {\n",
    "    \"timestamp\": utc_ts(),\n",
    "    \"source\": f\"s3://{FORM_BUCKET}/{FORM_IMAGES_PREFIX}/\",\n",
    "    \"total_images_scanned\": count_keys,\n",
    "    \"unique_recipe_ids\": len(img_ids),\n",
    "    \"total_images_matched\": total_images,\n",
    "    \"recipe_ids_with_images\": sorted(list(img_ids)),\n",
    "    \"recipe_to_images\": {rid: keys for rid, keys in id_to_imgkeys.items()}\n",
    "}\n",
    "\n",
    "# Save to local file\n",
    "with open(RECIPE_IDS_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(recipe_ids_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[OK] saved recipe IDs to {RECIPE_IDS_FILE}\")\n",
    "print(f\"[INFO] {len(img_ids)} recipe IDs will be used for document filtering\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a95942",
   "metadata": {},
   "source": [
    "## 5. Quality screening and per-recipe deduplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00437919",
   "metadata": {},
   "source": [
    "**This step adds:**\n",
    "\n",
    "- integrity check (corruption)\n",
    "- min size and aspect-range checks\n",
    "- optional blur screen (auto-disabled if OpenCV not installed)\n",
    "- per-recipe near-duplicate removal via perceptual hash\n",
    "- structured skips_log for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4efdea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screening images for quality and duplicates...\n",
      "[STATS] quality screening: {'evaluated': 50, 'kept': 46, 'corrupted': 0, 'too_small': 0, 'bad_aspect': 0, 'too_blurry': 3, 'dupes_removed': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Screening images for quality and duplicates...\")\n",
    "\n",
    "quality_stats = {\n",
    "    \"evaluated\": 0,\n",
    "    \"kept\": 0,\n",
    "    \"corrupted\": 0,\n",
    "    \"too_small\": 0,\n",
    "    \"bad_aspect\": 0,\n",
    "    \"too_blurry\": 0,\n",
    "    \"dupes_removed\": 0,\n",
    "}\n",
    "\n",
    "# Keep decisions per recipe id\n",
    "kept_per_rid: Dict[str, List[str]] = {}\n",
    "skips_log: List[dict] = []\n",
    "\n",
    "# For dedupe: track perceptual hashes per recipe\n",
    "seen_phashes: Dict[str, dict] = {}\n",
    "\n",
    "for rid, keys in id_to_imgkeys.items():\n",
    "    kept_per_rid[rid] = []\n",
    "    if DEDUPE_PER_RECIPE and imagehash is not None:\n",
    "        seen_phashes[rid] = {}\n",
    "\n",
    "    for src_key in keys:\n",
    "        quality_stats[\"evaluated\"] += 1\n",
    "\n",
    "        # Load original image bytes\n",
    "        try:\n",
    "            obj = s3.get_object(Bucket=FORM_BUCKET, Key=src_key)\n",
    "            raw = obj[\"Body\"].read()\n",
    "            img = Image.open(io.BytesIO(raw))\n",
    "            img.load()\n",
    "        except Exception as e:\n",
    "            quality_stats[\"corrupted\"] += 1\n",
    "            skips_log.append({\"key\": src_key, \"reason\": f\"corrupted:{type(e).__name__}\"})\n",
    "            continue\n",
    "\n",
    "        # Basic metrics\n",
    "        m = compute_metrics(img)\n",
    "        w, h, aspect = m[\"w\"], m[\"h\"], m[\"aspect\"]\n",
    "\n",
    "        if w < MIN_W or h < MIN_H:\n",
    "            quality_stats[\"too_small\"] += 1\n",
    "            skips_log.append({\"key\": src_key, \"reason\": f\"too_small:{w}x{h}\"})\n",
    "            continue\n",
    "\n",
    "        if not (MIN_ASPECT <= aspect <= MAX_ASPECT):\n",
    "            quality_stats[\"bad_aspect\"] += 1\n",
    "            skips_log.append({\"key\": src_key, \"reason\": f\"bad_aspect:{aspect:.2f}\"})\n",
    "            continue\n",
    "\n",
    "        if CV2_AVAILABLE and BLUR_VARLAP_MIN is not None:\n",
    "            fm = m.get(\"blur_varlap\", 0.0)\n",
    "            if fm < BLUR_VARLAP_MIN:\n",
    "                quality_stats[\"too_blurry\"] += 1\n",
    "                skips_log.append({\"key\": src_key, \"reason\": f\"blurry:varLap={fm:.2f} < {BLUR_VARLAP_MIN}\"})\n",
    "                continue\n",
    "\n",
    "        # Per-recipe near-duplicate check via phash\n",
    "        if DEDUPE_PER_RECIPE and imagehash is not None:\n",
    "            ph = str(imagehash.phash(img))\n",
    "            if ph in seen_phashes[rid]:\n",
    "                original_key = seen_phashes[rid][ph]\n",
    "                quality_stats[\"dupes_removed\"] += 1\n",
    "                skips_log.append({\"key\": src_key, \"reason\": f\"duplicate_phash:{ph}\", \"original_key\": original_key})\n",
    "                continue\n",
    "            seen_phashes[rid][ph] = src_key\n",
    "\n",
    "        kept_per_rid[rid].append(src_key)\n",
    "        quality_stats[\"kept\"] += 1\n",
    "\n",
    "print(\"[STATS] quality screening:\", quality_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48ffae",
   "metadata": {},
   "source": [
    "## 5. Copy Images to Trusted Zone\n",
    "\n",
    "This step takes the **subset of images that passed the quality screen** and writes them into the Trusted Zone in a **canonical format**. We do not perform a raw copy. We normalize each image so that downstream components see consistent inputs.\n",
    "\n",
    "**What happens here**\n",
    "- Read each kept image from the Formatted Zone using its original key.\n",
    "- Convert the image to `RGB`.\n",
    "- Resize with **letterboxing** to a fixed canvas so aspect ratio is preserved.\n",
    "- Encode to `JPEG` with a fixed quality setting.\n",
    "- Write the normalized file to `trusted-zone/images/` using the **same basename** but with a `.jpg` extension.\n",
    "\n",
    "**Why we normalize**\n",
    "- A single mode and size simplifies model-agnostic processing and caching.\n",
    "- Letterboxing avoids deformation and keeps visual content intact.\n",
    "- JPEG reduces storage while keeping visual quality stable.\n",
    "\n",
    "**Canonical spec**\n",
    "- Target size: `512 Ã— 512` pixels\n",
    "- Color mode: `RGB`\n",
    "- Format: `JPEG`\n",
    "- Quality: `90`\n",
    "\n",
    "**Input set**\n",
    "- Only images present in `kept_per_rid` are processed. This list is produced by the previous step after integrity, size, aspect, blur, and near-duplicate checks.\n",
    "\n",
    "**Naming and traceability**\n",
    "- Output key: `trusted-zone/images/<original_basename_without_ext>.jpg`\n",
    "- The original basename is preserved so the mapping to the source remains evident.\n",
    "\n",
    "**Idempotency and flags**\n",
    "- `OVERWRITE=False` makes the step skip files that already exist in the Trusted Zone.\n",
    "- `DRY_RUN=True` prints the planned operations without writing any object.\n",
    "\n",
    "**Outputs**\n",
    "- Normalized images under `trusted-zone/images/`.\n",
    "- Per-file write counters are included in the final report as `images_copied_normalized` and `images_skipped`.\n",
    "\n",
    "**Notes**\n",
    "- If OpenCV is installed the blur screen is active. If not, the notebook still runs and simply omits that filter.\n",
    "- Per-recipe near-duplicate removal uses perceptual hashing when the `imagehash` package is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee91ee9",
   "metadata": {},
   "source": [
    "### Why we use image hashing\n",
    "\n",
    "We use image hashing to find and remove duplicate or nearly identical images. Recipes could include repeated pictures with different filenames, sizes, or slight edits. A perceptual hash gives each image a small numerical fingerprint based on its visual content, not its raw bytes. If two images look the same, their hashes will also be the same. This lets us keep only one clean copy per recipe and avoid storing redundant or repeated images in the Trusted Zone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f43e912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying images to Trusted Zone...\n",
      "[STATS] images copied=46 skipped=0\n"
     ]
    }
   ],
   "source": [
    "print(\"Copying images to Trusted Zone...\")\n",
    "\n",
    "copied = skipped = 0\n",
    "\n",
    "if DRY_RUN:\n",
    "    print(\"[DRY_RUN] Would copy the following images:\")\n",
    "    for rid, keys in kept_per_rid.items():\n",
    "        for src_key in keys:\n",
    "            dst_key = f\"{TRUST_IMAGES_PREFIX}/{PurePosixPath(src_key).name}\"\n",
    "            print(f\"  {src_key} -> {dst_key}\")\n",
    "    copied = total_images\n",
    "else:\n",
    "    # Normalize and write only the KEPT images from the quality screen\n",
    "    for rid, keys in kept_per_rid.items():\n",
    "        for src_key in keys:\n",
    "            # Destination uses same basename but normalized to .jpg\n",
    "            base = PurePosixPath(src_key).name\n",
    "            base_noext = base.rsplit(\".\", 1)[0]\n",
    "            dst_key = f\"{TRUST_IMAGES_PREFIX}/{base_noext}.jpg\"\n",
    "\n",
    "            if not OVERWRITE and s3_head(TRUST_BUCKET, dst_key) is not None:\n",
    "                print(f\"[SKIP] {dst_key} already exists\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Load original\n",
    "                obj = s3.get_object(Bucket=FORM_BUCKET, Key=src_key)\n",
    "                raw = obj[\"Body\"].read()\n",
    "                img = Image.open(io.BytesIO(raw))\n",
    "                img.load()\n",
    "\n",
    "                # Normalize\n",
    "                out_bytes = normalize_image(img)\n",
    "\n",
    "                # Write normalized JPG\n",
    "                s3.put_object(\n",
    "                    Bucket=TRUST_BUCKET,\n",
    "                    Key=dst_key,\n",
    "                    Body=out_bytes,\n",
    "                    ContentType=\"image/jpeg\"\n",
    "                )\n",
    "                copied += 1\n",
    "            except ClientError as e:\n",
    "                print(f\"[WARN] write failed {src_key} -> {dst_key}: {e}\")\n",
    "                skipped += 1\n",
    "\n",
    "\n",
    "print(f\"[STATS] images copied={copied} skipped={skipped}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec44ad",
   "metadata": {},
   "source": [
    "## 6. Generate Processing Report\n",
    "\n",
    "Finally, we generate a comprehensive report of the image processing step and save it to the Trusted Zone for audit and monitoring purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93805aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote CSV skips log -> s3://trusted-zone/reports/\n",
      "[OK] wrote JSON report -> s3://trusted-zone/reports/\n"
     ]
    }
   ],
   "source": [
    "report = {\n",
    "    \"timestamp\": utc_ts(),\n",
    "    \"processing_step\": \"images\",\n",
    "    \"source_images_prefix\": f\"s3://{FORM_BUCKET}/{FORM_IMAGES_PREFIX}/\",\n",
    "    \"destination_images_prefix\": f\"s3://{TRUST_BUCKET}/{TRUST_IMAGES_PREFIX}/\",\n",
    "\n",
    "    # From earlier steps\n",
    "    \"total_images_scanned\": count_keys,\n",
    "    \"unique_recipe_ids_with_images\": len(img_ids),\n",
    "    \"total_images_matched\": total_images,\n",
    "\n",
    "    # New: quality screen stats\n",
    "    \"quality_screen\": {\n",
    "        \"evaluated\": quality_stats[\"evaluated\"],\n",
    "        \"kept\": quality_stats[\"kept\"],\n",
    "        \"corrupted\": quality_stats[\"corrupted\"],\n",
    "        \"too_small\": quality_stats[\"too_small\"],\n",
    "        \"bad_aspect\": quality_stats[\"bad_aspect\"],\n",
    "        \"too_blurry\": quality_stats[\"too_blurry\"],\n",
    "        \"dupes_removed\": quality_stats[\"dupes_removed\"],\n",
    "        \"blur_threshold\": BLUR_VARLAP_MIN,\n",
    "        \"min_wh\": [MIN_W, MIN_H],\n",
    "        \"aspect_range\": [MIN_ASPECT, MAX_ASPECT],\n",
    "        \"dedupe_enabled\": DEDUPE_PER_RECIPE,\n",
    "    },\n",
    "\n",
    "    # Output write stats\n",
    "    \"images_copied_normalized\": copied,\n",
    "    \"images_skipped\": skipped,\n",
    "\n",
    "    # Notebook config snapshot for traceability\n",
    "    \"normalization\": {\n",
    "        \"target_size\": TARGET_SIZE,\n",
    "        \"target_mode\": TARGET_MODE,\n",
    "        \"target_format\": TARGET_FMT,\n",
    "        \"target_quality\": TARGET_QUALITY\n",
    "    },\n",
    "\n",
    "    \"recipe_ids_file\": RECIPE_IDS_FILE,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"overwrite\": OVERWRITE\n",
    "}\n",
    "\n",
    "# Save skips log CSV\n",
    "if not DRY_RUN and skips_log:\n",
    "    csv_buf = io.StringIO()\n",
    "    csv_buf.write(\"key,reason,original_key\\n\")\n",
    "    for row in skips_log:\n",
    "        key = row[\"key\"].replace(\",\", \" \")\n",
    "        reason = row[\"reason\"].replace(\",\", \" \")\n",
    "        original = row.get(\"original_key\", \"\").replace(\",\", \" \")\n",
    "        csv_buf.write(f\"{key},{reason},{original}\\n\")\n",
    "    s3.put_object(\n",
    "        Bucket=TRUST_BUCKET,\n",
    "        Key=f\"{TRUST_REPORT_PREFIX}/images_skips_{utc_ts()}.csv\",\n",
    "        Body=csv_buf.getvalue().encode(\"utf-8\"),\n",
    "        ContentType=\"text/csv\"\n",
    "    )\n",
    "    print(f\"[OK] wrote CSV skips log -> s3://{TRUST_BUCKET}/{TRUST_REPORT_PREFIX}/\")\n",
    "\n",
    "# Save main JSON report\n",
    "if not DRY_RUN:\n",
    "    s3.put_object(\n",
    "        Bucket=TRUST_BUCKET,\n",
    "        Key=f\"{TRUST_REPORT_PREFIX}/images_processing_{utc_ts()}.json\",\n",
    "        Body=json.dumps(report, ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "    print(f\"[OK] wrote JSON report -> s3://{TRUST_BUCKET}/{TRUST_REPORT_PREFIX}/\")\n",
    "else:\n",
    "    print(\"[DRY_RUN] Would save the following reports:\")\n",
    "    print(\"CSV rows:\", len(skips_log))\n",
    "    print(json.dumps(report, indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
