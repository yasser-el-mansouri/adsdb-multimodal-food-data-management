{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4b2b58",
   "metadata": {},
   "source": [
    "# Formatted Zone — Documents\n",
    "\n",
    "This notebook handles the **documents format processing** step for the Formatted Zone of our data pipeline.  \n",
    "Its primary goal is to:\n",
    "\n",
    "1. **Load documents** from the Landing Zone\n",
    "2. **Join all recipes** in a single file\n",
    "3. **Skip data** that is no relevant\n",
    "4. **Upload data** in a efficent way to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb0ef4",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3ef588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, re, json\n",
    "from decimal import Decimal\n",
    "from pathlib import PurePosixPath\n",
    "import boto3, ijson\n",
    "from dotenv import load_dotenv\n",
    "from botocore.config import Config\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DOC_SRC_BUCKET =  \"landing-zone\"\n",
    "DOC_SRC_PREFIX = \"persistent_landing/documents\"\n",
    "OUT_S3_BUCKET  = \"formatted-zone\"\n",
    "OUT_S3_KEY     = \"documents/recipes.jsonl\"\n",
    "\n",
    "SKIP_FIELDS = {\"url\", \"partition\"}\n",
    "\n",
    "DOC_EXTS = (\".json\", \".jsonl\", \".ndjson\")\n",
    "ALWAYS_TAG = True\n",
    "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
    "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
    "MINIO_ENDPOINT =os.getenv(\"MINIO_ENDPOINT\")\n",
    "\n",
    "session = boto3.session.Session(\n",
    "    aws_access_key_id=MINIO_USER,\n",
    "    aws_secret_access_key=MINIO_PASSWORD,\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "s3 = session.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a19965",
   "metadata": {},
   "source": [
    "This block loads environment variables and sets up MinIO (via **boto3**) to read raw documents and write a formatted output. It defines source and destination S3 paths, basic filters (file extensions to read and fields to skip), and a flag for tagging. Then it creates a boto3 session/client pointed at the MinIO endpoint using credentials from `.env`, so later steps can stream JSON/JSONL inputs from `landing-zone` and write the consolidated result to `formatted-zone/recipes.jsonl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110d3d2",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e45015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_builtin(x):\n",
    "    if isinstance(x, Decimal):\n",
    "        return int(x) if x == x.to_integral_value() else float(x)\n",
    "    if isinstance(x, dict):  return {k: to_builtin(v) for k, v in x.items()}\n",
    "    if isinstance(x, list):  return [to_builtin(v) for v in x]\n",
    "    return x\n",
    "\n",
    "\n",
    "def list_docs(bucket: str, prefix: str):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.endswith(\"/\"):\n",
    "                continue\n",
    "            if key.lower().endswith(DOC_EXTS):\n",
    "                yield key\n",
    "\n",
    "\n",
    "def iter_jsonl(bucket, key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    for line in obj[\"Body\"].iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        yield to_builtin(json.loads(line))\n",
    "\n",
    "\n",
    "def iter_json_array(bucket, key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    for item in ijson.items(obj[\"Body\"], \"item\"):\n",
    "        yield to_builtin(item)\n",
    "\n",
    "\n",
    "def detect_iter(bucket, key):\n",
    "    if key.lower().endswith((\".jsonl\", \".ndjson\")):\n",
    "        return iter_jsonl(bucket, key)\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    head = obj[\"Body\"].read(2048)\n",
    "    obj[\"Body\"].close()\n",
    "    if head.lstrip()[:1] == b\"[\":\n",
    "        return iter_json_array(bucket, key)\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = obj[\"Body\"].read()\n",
    "    try:\n",
    "        one = to_builtin(json.loads(body))\n",
    "        def _once():\n",
    "            if isinstance(one, dict):\n",
    "                yield one\n",
    "        return _once()\n",
    "    except Exception:\n",
    "        return iter(())\n",
    "\n",
    "\n",
    "def short_tag_from_key(key: str) -> str:\n",
    "    stem = PurePosixPath(key).stem\n",
    "    if \"__\" in stem:\n",
    "        cand = stem.split(\"__\")[-1]\n",
    "    else:\n",
    "        toks = re.split(r\"[-_$]+\", stem)\n",
    "        cand = toks[-1] if toks else stem\n",
    "    tag = re.sub(r\"[^\\w\\-]+\", \"_\", cand).strip(\"_\").lower()\n",
    "    return (tag[-40:] if len(tag) > 40 else tag) or \"file\"\n",
    "\n",
    "\n",
    "def merge_with_tags(dst: dict, src: dict, tag: str):\n",
    "    if not isinstance(src, dict):\n",
    "        return\n",
    "    for k, v in src.items():\n",
    "        if k in SKIP_FIELDS:\n",
    "            continue\n",
    "        if k == \"id\":\n",
    "            if \"id\" not in dst:\n",
    "                dst[\"id\"] = v\n",
    "            continue\n",
    "        base = f\"{k}__from_{tag}\" if ALWAYS_TAG else k\n",
    "        new_k = base\n",
    "        i = 2\n",
    "        while new_k in dst:\n",
    "            if not ALWAYS_TAG and new_k == k:\n",
    "                base = f\"{k}__from_{tag}\"\n",
    "                new_k = base\n",
    "                continue\n",
    "            new_k = f\"{base}_{i}\"\n",
    "            i += 1\n",
    "        dst[new_k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06992653",
   "metadata": {},
   "source": [
    "This block provides tools for reading and merging JSON documents stored in MinIO.\n",
    "\n",
    "The conversion function `to_builtin` standardizes data types, turning objects like `Decimal` into basic Python numbers. The iterators (`list_docs`, `iter_jsonl`, `iter_json_array`, and `detect_iter`) automatically detect the file format—JSON, JSONL, or NDJSON—and stream records efficiently, allowing the script to handle large datasets without loading everything into memory.\n",
    "\n",
    "`short_tag_from_key` extracts a short, clean tag from each file name, used to identify the source of data when merging. Finally, `merge_with_tags` combines multiple JSON records into one, renaming fields with a tag suffix to avoid collisions and preserve the origin of each field. This ensures consistent, traceable merging across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e03a6",
   "metadata": {},
   "source": [
    "## 3. Join recipes removing irrelevant data in a efficent strucutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da295b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 3 document file(s) under s3://landing-zone/persistent_landing/documents\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m tag = short_tag_from_key(key)\n\u001b[32m     11\u001b[39m relevant = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetect_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOC_SRC_BUCKET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrid\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36miter_json_array\u001b[39m\u001b[34m(bucket, key)\u001b[39m\n\u001b[32m     29\u001b[39m obj = s3.get_object(Bucket=bucket, Key=key)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m ijson.items(obj[\u001b[33m\"\u001b[39m\u001b[33mBody\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mitem\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mto_builtin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mto_builtin\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, Decimal):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x == x.to_integral_value() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(x)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[43mto_builtin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m x.items()}\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;28;01mreturn\u001b[39;00m [to_builtin(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mto_builtin\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x == x.to_integral_value() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(x)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_builtin(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m x.items()}\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;28;01mreturn\u001b[39;00m [to_builtin(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "keys = list(list_docs(DOC_SRC_BUCKET, DOC_SRC_PREFIX))\n",
    "if not keys:\n",
    "    raise RuntimeError(f\"No documents under s3://{DOC_SRC_BUCKET}/{DOC_SRC_PREFIX}\")\n",
    "print(f\"[INFO] Found {len(keys)} document file(s) under s3://{DOC_SRC_BUCKET}/{DOC_SRC_PREFIX}\")\n",
    "\n",
    "joined: dict[str, dict] = {}\n",
    "total_seen = 0\n",
    "\n",
    "for idx, key in enumerate(keys, 1):\n",
    "    tag = short_tag_from_key(key)\n",
    "    relevant = 0\n",
    "    for rec in detect_iter(DOC_SRC_BUCKET, key):\n",
    "        rid = rec.get(\"id\")\n",
    "        if not rid:\n",
    "            continue\n",
    "        if rid not in joined:\n",
    "            joined[rid] = {}\n",
    "        merge_with_tags(joined[rid], rec, tag)\n",
    "        relevant += 1\n",
    "        total_seen += 1\n",
    "    print(f\"[MERGE] ({idx}/{len(keys)}) {key} [tag={tag}] -> records processed: {relevant}\")\n",
    "\n",
    "print(f\"[STATS] Total records processed: {total_seen}\")\n",
    "print(f\"[STATS] Unique ids merged:       {len(joined)}\")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for rid in sorted(joined.keys()):\n",
    "    buf.write(json.dumps(joined[rid], ensure_ascii=False) + \"\\n\")\n",
    "payload = buf.getvalue().encode(\"utf-8\")\n",
    "\n",
    "s3.put_object(\n",
    "    Bucket=OUT_S3_BUCKET,\n",
    "    Key=OUT_S3_KEY,\n",
    "    Body=payload,\n",
    "    ContentType=\"application/x-ndjson\",\n",
    "    Metadata={\"source-prefix\": f\"s3://{DOC_SRC_BUCKET}/{DOC_SRC_PREFIX}\",\n",
    "                \"note\": \"sample join by id with tagged collisions; plain ndjson\"},\n",
    ")\n",
    "print(f\"[OK] Wrote JSONL sample to s3://{OUT_S3_BUCKET}/{OUT_S3_KEY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15f32c",
   "metadata": {},
   "source": [
    "This block gathers all document keys under the source prefix, fails fast if none are found, and then streams each file to merge records by `id`. For every file, it derives a short source tag and uses it to rename colliding fields, preserving provenance. The result is a dictionary keyed by `id` with tagged fields from all inputs.\n",
    "\n",
    "Finally, it serializes the merged data as **NDJSON** and writes it to `formatted-zone/documents/recipes.jsonl` with metadata about the source prefix. The NDJSON format is used because it is more compact than regular JSON, reducing storage size and improving processing efficiency when handling large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
